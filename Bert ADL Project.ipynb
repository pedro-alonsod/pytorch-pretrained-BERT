{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.circleci', '.git', '.github', '.gitignore', '.ipynb_checkpoints', 'askreddit_data.json', 'Bert ADL Project.ipynb', 'CONTRIBUTING.md', 'convert_tf_checkpoint_to_pytorch.py', 'dataset.py', 'detokenization.py', 'docker', 'docs', 'examples', 'hubconf.py', 'hubconfs', 'LICENSE', 'MANIFEST.in', 'masked_language_model.py', 'modeling.py', 'notebooks', 'optimization.py', 'outputs.html', 'pytorch_pretrained_bert', 'README.md', 'requirements.txt', 'samples', 'setup.py', 'tests', 'test_threads.txt', 'tokenization.py', 'train_threads.txt', 'uncased_askreddit_data.tsv', '__init__.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "# from torchsummary import summary\n",
    "import tokenization\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import logging\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import six\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchtext import data,datasets\n",
    "from torchtext.vocab import GloVe,FastText,CharNGram\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import sys\n",
    "import torchtext.data.dataset\n",
    "\n",
    "import tokenization\n",
    "from modeling import BertConfig, BertForMaskedLanguageModelling\n",
    "from optimization import BERTAdam\n",
    "from masked_language_model import notqdm, convert_tokens_to_features, LMProcessor, predict_masked_words, predict_next_words, improve_words_recursive\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('./askreddit_data.json', 'r'))\n",
    "\n",
    "# ask_reddit_data =  pd.read_json('./askreddit_data.json')\n",
    "# ask_reddit_data = json_normalize(data)\n",
    "uncased_ask_reddit_data = pd.read_csv('./uncased_askreddit_data.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission score</th>\n",
       "      <th>comment score</th>\n",
       "      <th>submission id</th>\n",
       "      <th>comment id</th>\n",
       "      <th>title (string #1)</th>\n",
       "      <th>comment (string#2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80513</td>\n",
       "      <td>25495</td>\n",
       "      <td>bn3bpz</td>\n",
       "      <td>en27b8v</td>\n",
       "      <td>would you support a mandatory environmental se...</td>\n",
       "      <td>not mandatory, but there was a voluntary progr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80513</td>\n",
       "      <td>4686</td>\n",
       "      <td>bn3bpz</td>\n",
       "      <td>en2anug</td>\n",
       "      <td>would you support a mandatory environmental se...</td>\n",
       "      <td>i feel like instead of forcing people to do so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80513</td>\n",
       "      <td>2041</td>\n",
       "      <td>bn3bpz</td>\n",
       "      <td>en254lj</td>\n",
       "      <td>would you support a mandatory environmental se...</td>\n",
       "      <td>i wouldn't make it mandatory but maybe optiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80513</td>\n",
       "      <td>608</td>\n",
       "      <td>bn3bpz</td>\n",
       "      <td>en2gacy</td>\n",
       "      <td>would you support a mandatory environmental se...</td>\n",
       "      <td>we already have americorps, where you serve fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80513</td>\n",
       "      <td>310</td>\n",
       "      <td>bn3bpz</td>\n",
       "      <td>en2p5ro</td>\n",
       "      <td>would you support a mandatory environmental se...</td>\n",
       "      <td>i feel like forcing people to do it will just ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   submission score  comment score submission id comment id  \\\n",
       "0             80513          25495        bn3bpz    en27b8v   \n",
       "1             80513           4686        bn3bpz    en2anug   \n",
       "2             80513           2041        bn3bpz    en254lj   \n",
       "3             80513            608        bn3bpz    en2gacy   \n",
       "4             80513            310        bn3bpz    en2p5ro   \n",
       "\n",
       "                                   title (string #1)  \\\n",
       "0  would you support a mandatory environmental se...   \n",
       "1  would you support a mandatory environmental se...   \n",
       "2  would you support a mandatory environmental se...   \n",
       "3  would you support a mandatory environmental se...   \n",
       "4  would you support a mandatory environmental se...   \n",
       "\n",
       "                                  comment (string#2)  \n",
       "0  not mandatory, but there was a voluntary progr...  \n",
       "1  i feel like instead of forcing people to do so...  \n",
       "2  i wouldn't make it mandatory but maybe optiona...  \n",
       "3  we already have americorps, where you serve fo...  \n",
       "4  i feel like forcing people to do it will just ...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncased_ask_reddit_data.head()\n",
    "# uncased_ask_reddit_data['submission id'] == 'bn3bpz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bkw3yd', 'br19bh', 'bovsdb', 'bqhfq9', 'bq2sd5', 'bqx0lw', 'bkseqj', 'buz6lq', 'budsip', 'boxktx', 'bltmmq', 'bnlib2', 'bri3ja', 'btfu8k', 'bu1s5i', 'bkh4qb', 'br87g0', 'bo6854', 'bvkru1', 'bv6o78', 'bndpre', 'blf1l9', 'bsg50t', 'bro41c', 'bsahi8', 'buluna', 'bttd2p', 'bntob1', 'bqltzq', 'bvdaci', 'bniexw', 'bmjk7w', 'bmlzqs', 'bv51tj', 'btb5si', 'bqg2xi', 'bty7a5', 'btexev', 'bkxl3u', 'bue7cv', 'bmyv30', 'bojhpr', 'bq7pbb', 'bqc3ib', 'bkkoq2', 'bkjqjj', 'bnqk9j', 'bn3ab7', 'bs3th0', 'blna2q', 'bsomsy', 'bmac4g', 'bu766u', 'bp9xm7', 'bkckiw', 'bm8dfx', 'buunu0', 'bpw8r2', 'blrjnj', 'bvgvkh', 'bljq8j', 'bls9ab', 'bobtwf', 'bpr8xn', 'btm0yy', 'bp182w', 'bskplh', 'bmt72c', 'brayzq', 'bvr285', 'brzuwa', 'bubm6g', 'brlti4', 'bquclv', 'bk8ypo', 'bt3zc7', 'blcbrl', 'boi2wy', 'buso57', 'bk6w6e', 'bo3owh', 'bn3bpz', 'bl2scw', 'bstmp7', 'bs6tqc', 'bncqlb', 'blzj3m', 'bugoyw', 'bmhwr8', 'bogywr', 'bmhq67'} 91\n",
      "['bmyv30', 'bsg50t', 'bm8dfx', 'bmhwr8', 'bkw3yd', 'bttd2p', 'bue7cv', 'bskplh', 'bltmmq', 'bkkoq2'] 10\n",
      "['bncqlb', 'bqg2xi', 'brzuwa', 'bobtwf', 'bri3ja', 'bq2sd5', 'bvkru1', 'bqltzq', 'blna2q', 'btm0yy', 'bkh4qb', 'bmt72c', 'boi2wy', 'bniexw', 'bv51tj', 'bnlib2', 'bp182w', 'brayzq', 'bmjk7w', 'blcbrl', 'boxktx', 'bvr285', 'bqx0lw', 'bq7pbb', 'bn3ab7', 'br87g0', 'bkjqjj', 'btb5si', 'buz6lq', 'bk6w6e', 'bntob1', 'bv6o78', 'bt3zc7', 'buluna', 'bp9xm7', 'btfu8k', 'bsahi8', 'bqhfq9', 'bo6854', 'bkxl3u', 'budsip', 'brlti4', 'bkseqj', 'bugoyw', 'bty7a5', 'bn3bpz', 'buunu0', 'bmhq67', 'bkckiw', 'bqc3ib', 'bsomsy', 'bnqk9j', 'bstmp7', 'bls9ab', 'bogywr', 'bquclv', 'bojhpr', 'blrjnj', 'bs3th0', 'bo3owh', 'bpw8r2', 'bmlzqs', 'blzj3m', 'bvgvkh', 'bpr8xn', 'buso57', 'btexev', 'bvdaci', 'br19bh', 'bu766u', 'blf1l9', 'bovsdb', 'bro41c', 'bs6tqc', 'bk8ypo', 'bl2scw', 'bndpre', 'bljq8j', 'bubm6g', 'bu1s5i', 'bmac4g'] 81\n"
     ]
    }
   ],
   "source": [
    "array_id = uncased_ask_reddit_data['submission id']\n",
    "\n",
    "set_of_ids = set(array_id)\n",
    "\n",
    "print(set_of_ids, len(set_of_ids))\n",
    "\n",
    "# file_train, file_test = train_test_split(list(set_of_ids), random_state=42, test_size=0.10)\n",
    "# print(file_test, len(file_test))\n",
    "# print(file_train, len(file_train))\n",
    "# with open('train_threads.txt', 'a+') as file_train_ids:\n",
    "#     for elem in file_train:\n",
    "#         file_train_ids.write(elem + '\\n')\n",
    "\n",
    "# # with open('test_threads.txt', 'a+') as file_test_ids:\n",
    "#     for elem in file_test:\n",
    "#         file_test_ids.write(elem + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncased_ask_reddit_data['submission id'] == 'bn3bpz'\n",
    "# uncased_ask_reddit_data.loc[uncased_ask_reddit_data['submission id'] == 'bn3bpz']\n",
    "train_df = [] #pd.DataFrame(columns=['submission score\tcomment score', 'submission id', 'comment id', 'title (string #1)', 'comment (string#2)'])\n",
    "test_df = []\n",
    "\n",
    "with open('train_threads.txt', 'r') as trt:\n",
    "    for elem in trt.readlines():\n",
    "#         print(elem.strip())\n",
    "#         print(uncased_ask_reddit_data.loc[uncased_ask_reddit_data['submission id'] == elem.strip()])\n",
    "        train_df.append(uncased_ask_reddit_data.loc[uncased_ask_reddit_data['submission id'] == elem.strip()])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_threads.txt', 'r') as tst:\n",
    "    for elem in tst.readlines():\n",
    "#         print(elem.strip())\n",
    "#         print(uncased_ask_reddit_data.loc[uncased_ask_reddit_data['submission id'] == elem.strip()])\n",
    "        test_df.append(uncased_ask_reddit_data.loc[uncased_ask_reddit_data['submission id'] == elem.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_df[:3])\n",
    "# train_df_parsed = pd.DataFrame(train_df[0])\n",
    "# train_df_parsed.to_csv(path_or_buf='./file_0.csv', sep='\\t')\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    train_df_parsed = pd.DataFrame(train_df[i])\n",
    "    train_df_parsed.to_csv(path_or_buf='./data/train/file_'+str(i)+'.csv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_df)):\n",
    "    test_df_parsed = pd.DataFrame(train_df[i])\n",
    "    test_df_parsed.to_csv(path_or_buf='./data/test/file_'+str(i)+'.csv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\pedalo\\.cache\\torch\\pytorch_pretrained_bert\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', 'henson', 'was', 'a', 'puppet', '##eer', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenized input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2040,  2001,  3958, 27227,  1029,   102,  3958,   103,  2001,\n",
      "          1037, 13997, 11510,   102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "print(tokens_tensor)\n",
    "print(segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at C:\\Users\\pedalo\\.cache\\torch\\pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file C:\\Users\\pedalo\\.cache\\torch\\pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir C:\\Users\\pedalo\\AppData\\Local\\Temp\\tmpdz3jazde\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "if torch.cuda.is_available() == True:\n",
    "    tokens_tensor = tokens_tensor.to('cuda')\n",
    "    segments_tensors = segments_tensors.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "# We have a hidden states for each of the 12 layers in model bert-base-uncased\n",
    "assert len(encoded_layers) == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at C:\\Users\\pedalo\\.cache\\torch\\pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file C:\\Users\\pedalo\\.cache\\torch\\pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir C:\\Users\\pedalo\\AppData\\Local\\Temp\\tmp0pphl80j\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): BertLayerNorm()\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
      "    )\n",
      "  )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n",
      "torch.Size([1, 14, 30522])\n",
      "27227\n",
      "henson\n",
      "name bert.embeddings.word_embeddings.weight\n",
      "name bert.embeddings.position_embeddings.weight\n",
      "name bert.embeddings.token_type_embeddings.weight\n",
      "name bert.embeddings.LayerNorm.weight\n",
      "name bert.embeddings.LayerNorm.bias\n",
      "name bert.encoder.layer.0.attention.self.query.weight\n",
      "name bert.encoder.layer.0.attention.self.query.bias\n",
      "name bert.encoder.layer.0.attention.self.key.weight\n",
      "name bert.encoder.layer.0.attention.self.key.bias\n",
      "name bert.encoder.layer.0.attention.self.value.weight\n",
      "name bert.encoder.layer.0.attention.self.value.bias\n",
      "name bert.encoder.layer.0.attention.output.dense.weight\n",
      "name bert.encoder.layer.0.attention.output.dense.bias\n",
      "name bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.0.intermediate.dense.weight\n",
      "name bert.encoder.layer.0.intermediate.dense.bias\n",
      "name bert.encoder.layer.0.output.dense.weight\n",
      "name bert.encoder.layer.0.output.dense.bias\n",
      "name bert.encoder.layer.0.output.LayerNorm.weight\n",
      "name bert.encoder.layer.0.output.LayerNorm.bias\n",
      "name bert.encoder.layer.1.attention.self.query.weight\n",
      "name bert.encoder.layer.1.attention.self.query.bias\n",
      "name bert.encoder.layer.1.attention.self.key.weight\n",
      "name bert.encoder.layer.1.attention.self.key.bias\n",
      "name bert.encoder.layer.1.attention.self.value.weight\n",
      "name bert.encoder.layer.1.attention.self.value.bias\n",
      "name bert.encoder.layer.1.attention.output.dense.weight\n",
      "name bert.encoder.layer.1.attention.output.dense.bias\n",
      "name bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.1.intermediate.dense.weight\n",
      "name bert.encoder.layer.1.intermediate.dense.bias\n",
      "name bert.encoder.layer.1.output.dense.weight\n",
      "name bert.encoder.layer.1.output.dense.bias\n",
      "name bert.encoder.layer.1.output.LayerNorm.weight\n",
      "name bert.encoder.layer.1.output.LayerNorm.bias\n",
      "name bert.encoder.layer.2.attention.self.query.weight\n",
      "name bert.encoder.layer.2.attention.self.query.bias\n",
      "name bert.encoder.layer.2.attention.self.key.weight\n",
      "name bert.encoder.layer.2.attention.self.key.bias\n",
      "name bert.encoder.layer.2.attention.self.value.weight\n",
      "name bert.encoder.layer.2.attention.self.value.bias\n",
      "name bert.encoder.layer.2.attention.output.dense.weight\n",
      "name bert.encoder.layer.2.attention.output.dense.bias\n",
      "name bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.2.intermediate.dense.weight\n",
      "name bert.encoder.layer.2.intermediate.dense.bias\n",
      "name bert.encoder.layer.2.output.dense.weight\n",
      "name bert.encoder.layer.2.output.dense.bias\n",
      "name bert.encoder.layer.2.output.LayerNorm.weight\n",
      "name bert.encoder.layer.2.output.LayerNorm.bias\n",
      "name bert.encoder.layer.3.attention.self.query.weight\n",
      "name bert.encoder.layer.3.attention.self.query.bias\n",
      "name bert.encoder.layer.3.attention.self.key.weight\n",
      "name bert.encoder.layer.3.attention.self.key.bias\n",
      "name bert.encoder.layer.3.attention.self.value.weight\n",
      "name bert.encoder.layer.3.attention.self.value.bias\n",
      "name bert.encoder.layer.3.attention.output.dense.weight\n",
      "name bert.encoder.layer.3.attention.output.dense.bias\n",
      "name bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.3.intermediate.dense.weight\n",
      "name bert.encoder.layer.3.intermediate.dense.bias\n",
      "name bert.encoder.layer.3.output.dense.weight\n",
      "name bert.encoder.layer.3.output.dense.bias\n",
      "name bert.encoder.layer.3.output.LayerNorm.weight\n",
      "name bert.encoder.layer.3.output.LayerNorm.bias\n",
      "name bert.encoder.layer.4.attention.self.query.weight\n",
      "name bert.encoder.layer.4.attention.self.query.bias\n",
      "name bert.encoder.layer.4.attention.self.key.weight\n",
      "name bert.encoder.layer.4.attention.self.key.bias\n",
      "name bert.encoder.layer.4.attention.self.value.weight\n",
      "name bert.encoder.layer.4.attention.self.value.bias\n",
      "name bert.encoder.layer.4.attention.output.dense.weight\n",
      "name bert.encoder.layer.4.attention.output.dense.bias\n",
      "name bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.4.intermediate.dense.weight\n",
      "name bert.encoder.layer.4.intermediate.dense.bias\n",
      "name bert.encoder.layer.4.output.dense.weight\n",
      "name bert.encoder.layer.4.output.dense.bias\n",
      "name bert.encoder.layer.4.output.LayerNorm.weight\n",
      "name bert.encoder.layer.4.output.LayerNorm.bias\n",
      "name bert.encoder.layer.5.attention.self.query.weight\n",
      "name bert.encoder.layer.5.attention.self.query.bias\n",
      "name bert.encoder.layer.5.attention.self.key.weight\n",
      "name bert.encoder.layer.5.attention.self.key.bias\n",
      "name bert.encoder.layer.5.attention.self.value.weight\n",
      "name bert.encoder.layer.5.attention.self.value.bias\n",
      "name bert.encoder.layer.5.attention.output.dense.weight\n",
      "name bert.encoder.layer.5.attention.output.dense.bias\n",
      "name bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.5.intermediate.dense.weight\n",
      "name bert.encoder.layer.5.intermediate.dense.bias\n",
      "name bert.encoder.layer.5.output.dense.weight\n",
      "name bert.encoder.layer.5.output.dense.bias\n",
      "name bert.encoder.layer.5.output.LayerNorm.weight\n",
      "name bert.encoder.layer.5.output.LayerNorm.bias\n",
      "name bert.encoder.layer.6.attention.self.query.weight\n",
      "name bert.encoder.layer.6.attention.self.query.bias\n",
      "name bert.encoder.layer.6.attention.self.key.weight\n",
      "name bert.encoder.layer.6.attention.self.key.bias\n",
      "name bert.encoder.layer.6.attention.self.value.weight\n",
      "name bert.encoder.layer.6.attention.self.value.bias\n",
      "name bert.encoder.layer.6.attention.output.dense.weight\n",
      "name bert.encoder.layer.6.attention.output.dense.bias\n",
      "name bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.6.intermediate.dense.weight\n",
      "name bert.encoder.layer.6.intermediate.dense.bias\n",
      "name bert.encoder.layer.6.output.dense.weight\n",
      "name bert.encoder.layer.6.output.dense.bias\n",
      "name bert.encoder.layer.6.output.LayerNorm.weight\n",
      "name bert.encoder.layer.6.output.LayerNorm.bias\n",
      "name bert.encoder.layer.7.attention.self.query.weight\n",
      "name bert.encoder.layer.7.attention.self.query.bias\n",
      "name bert.encoder.layer.7.attention.self.key.weight\n",
      "name bert.encoder.layer.7.attention.self.key.bias\n",
      "name bert.encoder.layer.7.attention.self.value.weight\n",
      "name bert.encoder.layer.7.attention.self.value.bias\n",
      "name bert.encoder.layer.7.attention.output.dense.weight\n",
      "name bert.encoder.layer.7.attention.output.dense.bias\n",
      "name bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.7.intermediate.dense.weight\n",
      "name bert.encoder.layer.7.intermediate.dense.bias\n",
      "name bert.encoder.layer.7.output.dense.weight\n",
      "name bert.encoder.layer.7.output.dense.bias\n",
      "name bert.encoder.layer.7.output.LayerNorm.weight\n",
      "name bert.encoder.layer.7.output.LayerNorm.bias\n",
      "name bert.encoder.layer.8.attention.self.query.weight\n",
      "name bert.encoder.layer.8.attention.self.query.bias\n",
      "name bert.encoder.layer.8.attention.self.key.weight\n",
      "name bert.encoder.layer.8.attention.self.key.bias\n",
      "name bert.encoder.layer.8.attention.self.value.weight\n",
      "name bert.encoder.layer.8.attention.self.value.bias\n",
      "name bert.encoder.layer.8.attention.output.dense.weight\n",
      "name bert.encoder.layer.8.attention.output.dense.bias\n",
      "name bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.8.intermediate.dense.weight\n",
      "name bert.encoder.layer.8.intermediate.dense.bias\n",
      "name bert.encoder.layer.8.output.dense.weight\n",
      "name bert.encoder.layer.8.output.dense.bias\n",
      "name bert.encoder.layer.8.output.LayerNorm.weight\n",
      "name bert.encoder.layer.8.output.LayerNorm.bias\n",
      "name bert.encoder.layer.9.attention.self.query.weight\n",
      "name bert.encoder.layer.9.attention.self.query.bias\n",
      "name bert.encoder.layer.9.attention.self.key.weight\n",
      "name bert.encoder.layer.9.attention.self.key.bias\n",
      "name bert.encoder.layer.9.attention.self.value.weight\n",
      "name bert.encoder.layer.9.attention.self.value.bias\n",
      "name bert.encoder.layer.9.attention.output.dense.weight\n",
      "name bert.encoder.layer.9.attention.output.dense.bias\n",
      "name bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.9.intermediate.dense.weight\n",
      "name bert.encoder.layer.9.intermediate.dense.bias\n",
      "name bert.encoder.layer.9.output.dense.weight\n",
      "name bert.encoder.layer.9.output.dense.bias\n",
      "name bert.encoder.layer.9.output.LayerNorm.weight\n",
      "name bert.encoder.layer.9.output.LayerNorm.bias\n",
      "name bert.encoder.layer.10.attention.self.query.weight\n",
      "name bert.encoder.layer.10.attention.self.query.bias\n",
      "name bert.encoder.layer.10.attention.self.key.weight\n",
      "name bert.encoder.layer.10.attention.self.key.bias\n",
      "name bert.encoder.layer.10.attention.self.value.weight\n",
      "name bert.encoder.layer.10.attention.self.value.bias\n",
      "name bert.encoder.layer.10.attention.output.dense.weight\n",
      "name bert.encoder.layer.10.attention.output.dense.bias\n",
      "name bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.10.intermediate.dense.weight\n",
      "name bert.encoder.layer.10.intermediate.dense.bias\n",
      "name bert.encoder.layer.10.output.dense.weight\n",
      "name bert.encoder.layer.10.output.dense.bias\n",
      "name bert.encoder.layer.10.output.LayerNorm.weight\n",
      "name bert.encoder.layer.10.output.LayerNorm.bias\n",
      "name bert.encoder.layer.11.attention.self.query.weight\n",
      "name bert.encoder.layer.11.attention.self.query.bias\n",
      "name bert.encoder.layer.11.attention.self.key.weight\n",
      "name bert.encoder.layer.11.attention.self.key.bias\n",
      "name bert.encoder.layer.11.attention.self.value.weight\n",
      "name bert.encoder.layer.11.attention.self.value.bias\n",
      "name bert.encoder.layer.11.attention.output.dense.weight\n",
      "name bert.encoder.layer.11.attention.output.dense.bias\n",
      "name bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.11.intermediate.dense.weight\n",
      "name bert.encoder.layer.11.intermediate.dense.bias\n",
      "name bert.encoder.layer.11.output.dense.weight\n",
      "name bert.encoder.layer.11.output.dense.bias\n",
      "name bert.encoder.layer.11.output.LayerNorm.weight\n",
      "name bert.encoder.layer.11.output.LayerNorm.bias\n",
      "name bert.pooler.dense.weight\n",
      "name bert.pooler.dense.bias\n",
      "name cls.predictions.bias\n",
      "name cls.predictions.transform.dense.weight\n",
      "name cls.predictions.transform.dense.bias\n",
      "name cls.predictions.transform.LayerNorm.weight\n",
      "name cls.predictions.transform.LayerNorm.bias\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "print(model)\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "if torch.cuda.is_available() == True:\n",
    "    tokens_tensor = tokens_tensor.to('cuda')\n",
    "    segments_tensors = segments_tensors.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "    print(predictions.size())\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'henson'\n",
    "print(predicted_index)\n",
    "print(predicted_token)\n",
    "\n",
    "for (name, param) in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print('name', name#, 'weigth data', param.data\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "name fc.weight weigth data tensor([[ 0.0395,  0.0001, -0.0398,  ..., -0.0137,  0.0151, -0.0033],\n",
      "        [ 0.0112, -0.0344,  0.0121,  ...,  0.0254, -0.0233,  0.0096],\n",
      "        [-0.0254,  0.0164, -0.0430,  ...,  0.0147, -0.0373,  0.0021],\n",
      "        ...,\n",
      "        [ 0.0369,  0.0255,  0.0124,  ..., -0.0060,  0.0034,  0.0264],\n",
      "        [-0.0164,  0.0205,  0.0350,  ..., -0.0397,  0.0283, -0.0100],\n",
      "        [ 0.0056,  0.0002,  0.0368,  ..., -0.0351, -0.0242,  0.0194]])\n",
      "name fc.bias weigth data tensor([-0.0327,  0.0078,  0.0236,  0.0180, -0.0362, -0.0247,  0.0032, -0.0158])\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.vgg19(pretrained=True)\n",
    "print(model)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    # Replace the last fully-connected layer\n",
    "    # Parameters of newly constructed modules have requires_grad=True by default\n",
    "model.fc = nn.Linear(512, 8) # assuming that the fc7 layer has 512 neurons, otherwise change it \n",
    "# model.fc\n",
    "# model.cuda()\n",
    "# model.classifier[6] = nn.Linear(512, 8)\n",
    "# model.fc\n",
    "for (name, param) in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print('name', name, 'weigth data', param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
