{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.circleci', '.git', '.github', '.gitignore', '.ipynb_checkpoints', 'askreddit_data.json', 'Bert ADL Project.ipynb', 'CONTRIBUTING.md', 'convert_tf_checkpoint_to_pytorch.py', 'data', 'dataset.py', 'detokenization.py', 'docker', 'docs', 'examples', 'file_0.csv', 'hubconf.py', 'hubconfs', 'LICENSE', 'MANIFEST.in', 'masked_language_model.py', 'modeling.py', 'notebooks', 'optimization.py', 'outputs.html', 'pytorch_pretrained_bert', 'README.md', 'requirements.txt', 'samples', 'setup.py', 'tests', 'test_threads.txt', 'tokenization.py', 'train_threads.txt', 'uncased_askreddit_data.tsv', '__init__.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "# from torchsummary import summary\n",
    "import tokenization\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import logging\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import six\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchtext import data,datasets\n",
    "from torchtext.vocab import GloVe,FastText,CharNGram\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import sys\n",
    "import torchtext.data.dataset\n",
    "\n",
    "import tokenization\n",
    "from modeling import BertConfig, BertForMaskedLanguageModelling\n",
    "from optimization import BERTAdam\n",
    "from masked_language_model import notqdm, convert_tokens_to_features, LMProcessor, predict_masked_words, predict_next_words, improve_words_recursive\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "import csv\n",
    "\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('./askreddit_data.json', 'r'))\n",
    "\n",
    "# ask_reddit_data =  pd.read_json('./askreddit_data.json')\n",
    "# ask_reddit_data = json_normalize(data)\n",
    "uncased_ask_reddit_data = pd.read_csv('./uncased_askreddit_data.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission score</th>\n",
       "      <th>comment score</th>\n",
       "      <th>submission id</th>\n",
       "      <th>comment id</th>\n",
       "      <th>title (string #1)</th>\n",
       "      <th>comment (string#2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80513</td>\n",
       "      <td>25495</td>\n",
       "      <td>bn3bpz</td>\n",
       "      <td>en27b8v</td>\n",
       "      <td>would you support a mandatory environmental se...</td>\n",
       "      <td>not mandatory, but there was a voluntary progr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80513</td>\n",
       "      <td>4686</td>\n",
       "      <td>bn3bpz</td>\n",
       "      <td>en2anug</td>\n",
       "      <td>would you support a mandatory environmental se...</td>\n",
       "      <td>i feel like instead of forcing people to do so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80513</td>\n",
       "      <td>2041</td>\n",
       "      <td>bn3bpz</td>\n",
       "      <td>en254lj</td>\n",
       "      <td>would you support a mandatory environmental se...</td>\n",
       "      <td>i wouldn't make it mandatory but maybe optiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80513</td>\n",
       "      <td>608</td>\n",
       "      <td>bn3bpz</td>\n",
       "      <td>en2gacy</td>\n",
       "      <td>would you support a mandatory environmental se...</td>\n",
       "      <td>we already have americorps, where you serve fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80513</td>\n",
       "      <td>310</td>\n",
       "      <td>bn3bpz</td>\n",
       "      <td>en2p5ro</td>\n",
       "      <td>would you support a mandatory environmental se...</td>\n",
       "      <td>i feel like forcing people to do it will just ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   submission score  comment score submission id comment id  \\\n",
       "0             80513          25495        bn3bpz    en27b8v   \n",
       "1             80513           4686        bn3bpz    en2anug   \n",
       "2             80513           2041        bn3bpz    en254lj   \n",
       "3             80513            608        bn3bpz    en2gacy   \n",
       "4             80513            310        bn3bpz    en2p5ro   \n",
       "\n",
       "                                   title (string #1)  \\\n",
       "0  would you support a mandatory environmental se...   \n",
       "1  would you support a mandatory environmental se...   \n",
       "2  would you support a mandatory environmental se...   \n",
       "3  would you support a mandatory environmental se...   \n",
       "4  would you support a mandatory environmental se...   \n",
       "\n",
       "                                  comment (string#2)  \n",
       "0  not mandatory, but there was a voluntary progr...  \n",
       "1  i feel like instead of forcing people to do so...  \n",
       "2  i wouldn't make it mandatory but maybe optiona...  \n",
       "3  we already have americorps, where you serve fo...  \n",
       "4  i feel like forcing people to do it will just ...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncased_ask_reddit_data.head()\n",
    "# uncased_ask_reddit_data['submission id'] == 'bn3bpz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bkw3yd', 'br19bh', 'bovsdb', 'bqhfq9', 'bq2sd5', 'bqx0lw', 'bkseqj', 'buz6lq', 'budsip', 'boxktx', 'bltmmq', 'bnlib2', 'bri3ja', 'btfu8k', 'bu1s5i', 'bkh4qb', 'br87g0', 'bo6854', 'bvkru1', 'bv6o78', 'bndpre', 'blf1l9', 'bsg50t', 'bro41c', 'bsahi8', 'buluna', 'bttd2p', 'bntob1', 'bqltzq', 'bvdaci', 'bniexw', 'bmjk7w', 'bmlzqs', 'bv51tj', 'btb5si', 'bqg2xi', 'bty7a5', 'btexev', 'bkxl3u', 'bue7cv', 'bmyv30', 'bojhpr', 'bq7pbb', 'bqc3ib', 'bkkoq2', 'bkjqjj', 'bnqk9j', 'bn3ab7', 'bs3th0', 'blna2q', 'bsomsy', 'bmac4g', 'bu766u', 'bp9xm7', 'bkckiw', 'bm8dfx', 'buunu0', 'bpw8r2', 'blrjnj', 'bvgvkh', 'bljq8j', 'bls9ab', 'bobtwf', 'bpr8xn', 'btm0yy', 'bp182w', 'bskplh', 'bmt72c', 'brayzq', 'bvr285', 'brzuwa', 'bubm6g', 'brlti4', 'bquclv', 'bk8ypo', 'bt3zc7', 'blcbrl', 'boi2wy', 'buso57', 'bk6w6e', 'bo3owh', 'bn3bpz', 'bl2scw', 'bstmp7', 'bs6tqc', 'bncqlb', 'blzj3m', 'bugoyw', 'bmhwr8', 'bogywr', 'bmhq67'} 91\n",
      "['bmyv30', 'bsg50t', 'bm8dfx', 'bmhwr8', 'bkw3yd', 'bttd2p', 'bue7cv', 'bskplh', 'bltmmq', 'bkkoq2'] 10\n",
      "['bncqlb', 'bqg2xi', 'brzuwa', 'bobtwf', 'bri3ja', 'bq2sd5', 'bvkru1', 'bqltzq', 'blna2q', 'btm0yy', 'bkh4qb', 'bmt72c', 'boi2wy', 'bniexw', 'bv51tj', 'bnlib2', 'bp182w', 'brayzq', 'bmjk7w', 'blcbrl', 'boxktx', 'bvr285', 'bqx0lw', 'bq7pbb', 'bn3ab7', 'br87g0', 'bkjqjj', 'btb5si', 'buz6lq', 'bk6w6e', 'bntob1', 'bv6o78', 'bt3zc7', 'buluna', 'bp9xm7', 'btfu8k', 'bsahi8', 'bqhfq9', 'bo6854', 'bkxl3u', 'budsip', 'brlti4', 'bkseqj', 'bugoyw', 'bty7a5', 'bn3bpz', 'buunu0', 'bmhq67', 'bkckiw', 'bqc3ib', 'bsomsy', 'bnqk9j', 'bstmp7', 'bls9ab', 'bogywr', 'bquclv', 'bojhpr', 'blrjnj', 'bs3th0', 'bo3owh', 'bpw8r2', 'bmlzqs', 'blzj3m', 'bvgvkh', 'bpr8xn', 'buso57', 'btexev', 'bvdaci', 'br19bh', 'bu766u', 'blf1l9', 'bovsdb', 'bro41c', 'bs6tqc', 'bk8ypo', 'bl2scw', 'bndpre', 'bljq8j', 'bubm6g', 'bu1s5i', 'bmac4g'] 81\n"
     ]
    }
   ],
   "source": [
    "array_id = uncased_ask_reddit_data['submission id']\n",
    "\n",
    "set_of_ids = set(array_id)\n",
    "\n",
    "print(set_of_ids, len(set_of_ids))\n",
    "\n",
    "# file_train, file_test = train_test_split(list(set_of_ids), random_state=42, test_size=0.10)\n",
    "# print(file_test, len(file_test))\n",
    "# print(file_train, len(file_train))\n",
    "# with open('train_threads.txt', 'a+') as file_train_ids:\n",
    "#     for elem in file_train:\n",
    "#         file_train_ids.write(elem + '\\n')\n",
    "\n",
    "# # with open('test_threads.txt', 'a+') as file_test_ids:\n",
    "#     for elem in file_test:\n",
    "#         file_test_ids.write(elem + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncased_ask_reddit_data['submission id'] == 'bn3bpz'\n",
    "# uncased_ask_reddit_data.loc[uncased_ask_reddit_data['submission id'] == 'bn3bpz']\n",
    "train_df = [] #pd.DataFrame(columns=['submission score\tcomment score', 'submission id', 'comment id', 'title (string #1)', 'comment (string#2)'])\n",
    "test_df = []\n",
    "\n",
    "with open('train_threads.txt', 'r') as trt:\n",
    "    for elem in trt.readlines():\n",
    "#         print(elem.strip())\n",
    "#         print(uncased_ask_reddit_data.loc[uncased_ask_reddit_data['submission id'] == elem.strip()])\n",
    "        train_df.append(uncased_ask_reddit_data.loc[uncased_ask_reddit_data['submission id'] == elem.strip()])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_threads.txt', 'r') as tst:\n",
    "    for elem in tst.readlines():\n",
    "#         print(elem.strip())\n",
    "#         print(uncased_ask_reddit_data.loc[uncased_ask_reddit_data['submission id'] == elem.strip()])\n",
    "        test_df.append(uncased_ask_reddit_data.loc[uncased_ask_reddit_data['submission id'] == elem.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_df[:3])\n",
    "# train_df_parsed = pd.DataFrame(train_df[0])\n",
    "# train_df_parsed.to_csv(path_or_buf='./file_0.csv', sep='\\t')\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    train_df_parsed = pd.DataFrame(train_df[i])\n",
    "    train_df_parsed.to_csv(path_or_buf='./data/train/file_'+str(i)+'.csv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_df)):\n",
    "    test_df_parsed = pd.DataFrame(train_df[i])\n",
    "    test_df_parsed.to_csv(path_or_buf='./data/test/file_'+str(i)+'.csv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([['36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634', '36634'], ['3112', '2949', '1087', '12607', '10355', '4568', '19215', '6393', '5308', '19319', '3043', '32833', '13020', '8011', '531', '7490', '17465', '15836', '33470', '6062', '2768', '1747', '10499', '118', '428', '3958', '14732', '857', '10291', '910', '2891', '1747', '833', '558', '348', '280', '739', '2028', '127', '223', '210', '503', '287', '1483', '103', '48', '186', '120', '435', '84', '79', '165', '592', '35', '561', '183', '263', '166', '31', '135', '111', '121', '29', '28'], ['bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g', 'bmac4g'], ['emv58aa', 'emvhxhr', 'emvl442', 'emvcwx4', 'emvaz4e', 'emvdpu3', 'emv92ks', 'emvkd9v', 'emvdufc', 'emvay48', 'emv5xft', 'emvbeoq', 'emv9ely', 'emv8h37', 'emvd9ml', 'emv4adj', 'emv1mhf', 'emv215e', 'emvhke8', 'emv1l5a', 'emv1nww', 'emv9ar3', 'emv4w3e', 'emvjcx2', 'emv42rp', 'emv80x2', 'emv1spn', 'emvioip', 'emv4azk', 'emvgybx', 'emv208d', 'emve9uq', 'emv3mh4', 'emv5kxz', 'emvbygr', 'emv55qy', 'emvadgs', 'emv6o43', 'emvdonh', 'emvhewk', 'emvbldp', 'emvixj5', 'emvnobt', 'emvb4m7', 'emvkedl', 'emvvi1l', 'emvqqr5', 'emvazr6', 'emvk2xr', 'emvcfxv', 'emw18zn', 'emvdrot', 'emv9vyv', 'emvz0z4', 'emv1zia', 'emv97oh', 'emvjv8h', 'emv8jnq', 'emw12su', 'emv5y78', 'emv5kcm', 'emvg938', 'emw7f18', 'emw8air'], ['whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?', 'whats something that cant be explained, it must be experienced?'], [\"addiction. i've seen many people ask for a description and i've seen a lot of good explanations but i don't think any words really capture it right.\", 'that feeling when your nose is clogged but then you get into the right position and your nose frees up', 'that feeling when you stare at the stars and realize how small we are on an astral level.', \"the effect of chronic pain on one's mental health.\", 'psychosis', 'going up the stairs and putting your foot down thinking theres one more step at the top than there actually is.', '\"hot\" you can\\'t explain it to a child. they have to experience it to understand.', \"i'm going to say the realization of your own mortality. it's always an obscure concept that always seems so far away until in one terrifying moment it becomes a crystal clear fact of reality.\", 'impostor syndrome, at least for me when i try to explain to people', \"the moment when you are playing an instrument and you aren't really making decisions on what you are playing. the music just flows out.\", 'i want to say virtual reality. lots of people think its a gimmick without trying it.', \"dream logic/chronology. sometimes you can't put into words what happened in your dream, or how two things were true at the same time. but when you experience it, it makes perfect sense.\", 'its hard to explain an anxiety attack unless youve had one. my mother use to get them and i never understood what she was going through until i started having them later on in life.', 'getting old. it just plain sucks and nothing can prepare you for it. your body starts doing weird shit. everything hurts. your brain starts going and you actually can recognize it but damn it just is depressing.', 'exercise feels good after the initial few months. edit: thanks for the silver, kind stranger.', 'the pain of losing a loved one', 'color.', \"when you fall for a terrible person and gloss over all of their flaws. doesn't matter how hard your friends try to explain\", \"nostalgia. it's so much more than just missing the past, it's a very strange blend of sad and happy\", 'drugs (halucinating ones)', 'scuba diving', 'a full solar eclipse.', 'an orgasm.', 'having a learning disorder or any disorder that inhibits your cognitive function. there is really no way to properly explain what it\\'s like to have minimal visuospacial processing, or no working memory, or a slow processing speed, or any of the deficits that come with it. it\\'s more than \"my brain doesn\\'t work\" or \"i struggle at this.\" the best way i can try to explain something like the lack of visuospacial skills is to ask someone how many meters away an object is--but even then, it\\'s not a complete comparison. they still have a rough idea of where that object is, and may be able to roughly translate feet to meters.', \"i really don't know how to describe it but it's a weird taste in my throat that comes randomly from nowhere\", 'being blind. trying to understand that there\\'s \"nothing\" for a blind person and that it isn\\'t just \"darkness/black\" hurts my brain to try and understand edit: please stop saying \"imagine trying to look out of [body part].\" it doesn\\'t fucking help', 'im struggling to find the words to tell you, to be honest.', \"just how large the grand canyon is. people can tell you its x miles long and y miles deep and z miles wide but you can't really comprehend what it's like to stand on the edge of something like that.\", 'being in love with a person that is in love with you as well. edit: thanks for all the upvotes and rewards, i didnt think people felt the same', 'not feeling good enough? you trap yourself in a prison built from your own thoughts, and you are overwhelmed with so many emotions that i cannot possibly put into words.', 'depression.', 'childbirth &#x200b; divorce', 'standing on a pier or in a boat and seeing nothing but ocean surrounding you on all sides.', \"what it feels like to experience an ocean to someone that's never seen a body of water greater than a small lake.\", 'lucid dreaming', 'mania', 'finally feeling like you are an adult. the best i can say is it does happen, and it happens after the time you get the \"no one knows anything\" epiphany, and it doesn\\'t happen for everyone. but once you get there you know. it\\'s like a general comfort and confidence, but those words mean nothing unless you\\'ve experienced what i\\'m trying to describe.', \"reddit to people that don't use reddit.\", 'the feeling of being shot. it seems to vary so much person to person if you ever hear/read it described. obviously the location of the bullet wound would be a factor but its something i dont think anyone can understand without experiencing it.', 'having a dad thats there for you for the first 13 years and then one day he just leaves your life, but he still calls and interacts with your brother.', 'breaking up with a partner you still love. tying a neck tie. how to drive safely in the snow.', \"being in an abusive relationship and not realizing it's abusive. or, realizing you're in an abusive relationship and not wanting to leave.\", 'adhd. you can tell people how you feel and how your brain works, but its usually met by oh i do that sometimes, maybe i have it? or an awkward laugh and a blank stare because they cant relate. how on earth do i explain the panic attacks, loss of focus, the feeling of spiraling, the loneliness, the hyper focus, rejection sensitivity, heightened feelings (like i really wish i didnt feel my feelings so deeply), etc. its very hard to explain and my fianc is still dealing with trying to understand all sides to it. theres also the desperate feeling of wanting to be normal, and trying to tell myself theres nothing wrong with me.', \"people look at me funny when i say this, but combat. after i returned from overseas people would ask me what it was like, but i honestly couldn't explain it in a way that they could really understand. the risks far out weight the rewards, but i gather that if you talked to a lot of veterans who saw combat, *many* of them will look upon it almost fondly. there's nothing like it. no amount of skydiving or other extreme sports come close to the exhilaration and rush of being shot at and retuning fire. it's an experience unmatched by anything in the world, where the only difference between life and death is if you're better at making subtle aiming changes. sounds bloodthirsty, and i'm not saying it's a fun thing to kill people or see others get killed, it's just one of those things that you have to experience to really understand what i'm talking about.\", \"seeing mountains for the first time. pictures don't do it justice. i live in a city no where near mountains, but when i went to alberta last year and saw the rocky mountains from banff for the first time, shit was so surreal and i just can't get over how beautiful mountains are\", 'migraines. \"imagine a really bad headache on one side of your head, behind your eye\" doesn\\'t cut it when you\\'re smelling weird smells, seeing things that aren\\'t there and physically nauseous from a headache. they\\'re the worst and so hard to explain unless you\\'ve have one.', 'having milk come out of your boobs. there is no way to accurately describes this to a woman who has not experienced it.', 'skydiving. feeling the stomach churning drop and instant spike of adrenaline. followed by the amazing feeling of weightlessness. words could never do it justice.', \"being a parent. you've spent your whole life as half of a parent-child relationship and you think you've got it worked out. hell, you even have a dog and there's \\\\_that\\\\_ relationship. but nothing prepares you for the amount of love, the i-would-happily-die-to-protect-this-creature, and the responsibility. it's amazing and you just don't get it until it happens to you -- at least i didn't.\", 'bee stings . been stung 6 times but can never explain how it feels , its always bugged me . &#x200b; edit : wow ! i didnt know other people felt the same ! i just thought i had a really shit memory !', 'period cramps, its such a unique kind of pain.', 'hugging your mom after a particularly long, hard day', 'why people like math.', 'what its truly like to grieve for your departed baby. a lot of people sympathize, very few can truly empathize.', 'going to sleep after a physically exhausting day. &#x200b; also, weed.', \"marilyn monroe in movies. in pictures some people don't get why's she so popular. watch her in a movie, you'll know why.\", 'okay, disclaimer, dont experience this please. but, a suicide attempt. its really hard to explain what goes on the second before it happens. right before i tried overdosing, i felt extremely relaxed and at peace. its completely surreal. like i said, do not experience one of these, it sucks being in a mental hospital.', \"prostate orgasms for a guy. don't judge before you experience it\", 'living alone for an *extremely* long time. i\\'ve been living alone since i left college - over *forty years ago*. people still comment how great it must be to \"have the house to yourself\". they have absolutely no idea - *no idea* - what\\'s it\\'s like to come home to an empty house and spend the evening alone, night after night. and then the nights stretch into months and years, and then finally decades spent alone. i retired a few years ago so now i\\'ll often go for a few days without talking to anyone. some of the ramifications are just weird - for example my vocal chords have gotten weak from disuse so if i go to lunch with friends then the next day my throat is sore just from carrying on ordinary conversations. anyway this is why i love my dogs so much, at least i don\\'t feel completely alone. people think they do, but they really have absolutely *no idea* what it\\'s like.', 'dmt', 'being out on a pitch in front of 50,000 people.', 'shwarma', \"before i became addicted to heroin i thought addicts we're just weak. no real man, no good person, no morally sound individual would throw their life away. it should be easy to stop once you take a step back and see what's happening. i had an alcoholic friend who i thought was insane for needing to go to aa. i didn't know what addiction was until i got hooked on heroin. i feel like there's no way to really get it until you're gripped by the power. i still don't think it's a disease but i also don't think that it's caused by stupidity or weakness. it's a scary thing. if i really liked chocolate cake but it made me sick as fuck between bites i'd never touch the stuff, but somehow i couldn't apply that logic to opiates.\", 'broccoli, more specifically after it has been properly cooked. for 25 years i hated the shit. went to culinary school. learned what butter and salt can do. now im like the mother fucking witness to the broccoli revival. also helps i was diagnosed with type 1 diabetes and now am kept alive by almost vegetables entirely. anyway. broccoli. love you.']])"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dict = dict()\n",
    "data_dict['submission score'] = []\n",
    "data_dict['comment score'] = []\n",
    "data_dict['submission id'] = []\n",
    "data_dict['comment id'] = []\n",
    "data_dict['title (string #1)'] = []\n",
    "data_dict['comment (string#2)'] = []\n",
    "with open('./data/train/file_80.csv') as tsvfile:\n",
    "    reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
    "#     next(reader)\n",
    "#     df = pd.DataFrame(reader.data, columns=readr.columns)\n",
    "#     print(df.head())\n",
    "    for row in reader:\n",
    "#         print(row)\n",
    "#         data_dict += row\n",
    "        data_dict['submission score'].append(row['submission score'])\n",
    "        data_dict['comment score'].append(row['comment score'])\n",
    "        data_dict['submission id'].append(row['submission id'])\n",
    "        data_dict['comment id'].append(row['comment id'])\n",
    "        data_dict['title (string #1)'].append(row['title (string #1)'])\n",
    "        data_dict['comment (string#2)'].append(row['comment (string#2)'])\n",
    "        \n",
    "print(data_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\pedalo\\.cache\\torch\\pytorch_pretrained_bert\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', 'henson', 'was', 'a', 'puppet', '##eer', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenized input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2040,  2001,  3958, 27227,  1029,   102,  3958,   103,  2001,\n",
      "          1037, 13997, 11510,   102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "print(tokens_tensor)\n",
    "print(segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at C:\\Users\\pedalo\\.cache\\torch\\pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file C:\\Users\\pedalo\\.cache\\torch\\pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir C:\\Users\\pedalo\\AppData\\Local\\Temp\\tmpdz3jazde\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "if torch.cuda.is_available() == True:\n",
    "    tokens_tensor = tokens_tensor.to('cuda')\n",
    "    segments_tensors = segments_tensors.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "# We have a hidden states for each of the 12 layers in model bert-base-uncased\n",
    "assert len(encoded_layers) == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at C:\\Users\\pedalo\\.cache\\torch\\pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file C:\\Users\\pedalo\\.cache\\torch\\pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir C:\\Users\\pedalo\\AppData\\Local\\Temp\\tmp0pphl80j\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): BertLayerNorm()\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
      "    )\n",
      "  )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n",
      "torch.Size([1, 14, 30522])\n",
      "27227\n",
      "henson\n",
      "name bert.embeddings.word_embeddings.weight\n",
      "name bert.embeddings.position_embeddings.weight\n",
      "name bert.embeddings.token_type_embeddings.weight\n",
      "name bert.embeddings.LayerNorm.weight\n",
      "name bert.embeddings.LayerNorm.bias\n",
      "name bert.encoder.layer.0.attention.self.query.weight\n",
      "name bert.encoder.layer.0.attention.self.query.bias\n",
      "name bert.encoder.layer.0.attention.self.key.weight\n",
      "name bert.encoder.layer.0.attention.self.key.bias\n",
      "name bert.encoder.layer.0.attention.self.value.weight\n",
      "name bert.encoder.layer.0.attention.self.value.bias\n",
      "name bert.encoder.layer.0.attention.output.dense.weight\n",
      "name bert.encoder.layer.0.attention.output.dense.bias\n",
      "name bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.0.intermediate.dense.weight\n",
      "name bert.encoder.layer.0.intermediate.dense.bias\n",
      "name bert.encoder.layer.0.output.dense.weight\n",
      "name bert.encoder.layer.0.output.dense.bias\n",
      "name bert.encoder.layer.0.output.LayerNorm.weight\n",
      "name bert.encoder.layer.0.output.LayerNorm.bias\n",
      "name bert.encoder.layer.1.attention.self.query.weight\n",
      "name bert.encoder.layer.1.attention.self.query.bias\n",
      "name bert.encoder.layer.1.attention.self.key.weight\n",
      "name bert.encoder.layer.1.attention.self.key.bias\n",
      "name bert.encoder.layer.1.attention.self.value.weight\n",
      "name bert.encoder.layer.1.attention.self.value.bias\n",
      "name bert.encoder.layer.1.attention.output.dense.weight\n",
      "name bert.encoder.layer.1.attention.output.dense.bias\n",
      "name bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.1.intermediate.dense.weight\n",
      "name bert.encoder.layer.1.intermediate.dense.bias\n",
      "name bert.encoder.layer.1.output.dense.weight\n",
      "name bert.encoder.layer.1.output.dense.bias\n",
      "name bert.encoder.layer.1.output.LayerNorm.weight\n",
      "name bert.encoder.layer.1.output.LayerNorm.bias\n",
      "name bert.encoder.layer.2.attention.self.query.weight\n",
      "name bert.encoder.layer.2.attention.self.query.bias\n",
      "name bert.encoder.layer.2.attention.self.key.weight\n",
      "name bert.encoder.layer.2.attention.self.key.bias\n",
      "name bert.encoder.layer.2.attention.self.value.weight\n",
      "name bert.encoder.layer.2.attention.self.value.bias\n",
      "name bert.encoder.layer.2.attention.output.dense.weight\n",
      "name bert.encoder.layer.2.attention.output.dense.bias\n",
      "name bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.2.intermediate.dense.weight\n",
      "name bert.encoder.layer.2.intermediate.dense.bias\n",
      "name bert.encoder.layer.2.output.dense.weight\n",
      "name bert.encoder.layer.2.output.dense.bias\n",
      "name bert.encoder.layer.2.output.LayerNorm.weight\n",
      "name bert.encoder.layer.2.output.LayerNorm.bias\n",
      "name bert.encoder.layer.3.attention.self.query.weight\n",
      "name bert.encoder.layer.3.attention.self.query.bias\n",
      "name bert.encoder.layer.3.attention.self.key.weight\n",
      "name bert.encoder.layer.3.attention.self.key.bias\n",
      "name bert.encoder.layer.3.attention.self.value.weight\n",
      "name bert.encoder.layer.3.attention.self.value.bias\n",
      "name bert.encoder.layer.3.attention.output.dense.weight\n",
      "name bert.encoder.layer.3.attention.output.dense.bias\n",
      "name bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.3.intermediate.dense.weight\n",
      "name bert.encoder.layer.3.intermediate.dense.bias\n",
      "name bert.encoder.layer.3.output.dense.weight\n",
      "name bert.encoder.layer.3.output.dense.bias\n",
      "name bert.encoder.layer.3.output.LayerNorm.weight\n",
      "name bert.encoder.layer.3.output.LayerNorm.bias\n",
      "name bert.encoder.layer.4.attention.self.query.weight\n",
      "name bert.encoder.layer.4.attention.self.query.bias\n",
      "name bert.encoder.layer.4.attention.self.key.weight\n",
      "name bert.encoder.layer.4.attention.self.key.bias\n",
      "name bert.encoder.layer.4.attention.self.value.weight\n",
      "name bert.encoder.layer.4.attention.self.value.bias\n",
      "name bert.encoder.layer.4.attention.output.dense.weight\n",
      "name bert.encoder.layer.4.attention.output.dense.bias\n",
      "name bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.4.intermediate.dense.weight\n",
      "name bert.encoder.layer.4.intermediate.dense.bias\n",
      "name bert.encoder.layer.4.output.dense.weight\n",
      "name bert.encoder.layer.4.output.dense.bias\n",
      "name bert.encoder.layer.4.output.LayerNorm.weight\n",
      "name bert.encoder.layer.4.output.LayerNorm.bias\n",
      "name bert.encoder.layer.5.attention.self.query.weight\n",
      "name bert.encoder.layer.5.attention.self.query.bias\n",
      "name bert.encoder.layer.5.attention.self.key.weight\n",
      "name bert.encoder.layer.5.attention.self.key.bias\n",
      "name bert.encoder.layer.5.attention.self.value.weight\n",
      "name bert.encoder.layer.5.attention.self.value.bias\n",
      "name bert.encoder.layer.5.attention.output.dense.weight\n",
      "name bert.encoder.layer.5.attention.output.dense.bias\n",
      "name bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.5.intermediate.dense.weight\n",
      "name bert.encoder.layer.5.intermediate.dense.bias\n",
      "name bert.encoder.layer.5.output.dense.weight\n",
      "name bert.encoder.layer.5.output.dense.bias\n",
      "name bert.encoder.layer.5.output.LayerNorm.weight\n",
      "name bert.encoder.layer.5.output.LayerNorm.bias\n",
      "name bert.encoder.layer.6.attention.self.query.weight\n",
      "name bert.encoder.layer.6.attention.self.query.bias\n",
      "name bert.encoder.layer.6.attention.self.key.weight\n",
      "name bert.encoder.layer.6.attention.self.key.bias\n",
      "name bert.encoder.layer.6.attention.self.value.weight\n",
      "name bert.encoder.layer.6.attention.self.value.bias\n",
      "name bert.encoder.layer.6.attention.output.dense.weight\n",
      "name bert.encoder.layer.6.attention.output.dense.bias\n",
      "name bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.6.intermediate.dense.weight\n",
      "name bert.encoder.layer.6.intermediate.dense.bias\n",
      "name bert.encoder.layer.6.output.dense.weight\n",
      "name bert.encoder.layer.6.output.dense.bias\n",
      "name bert.encoder.layer.6.output.LayerNorm.weight\n",
      "name bert.encoder.layer.6.output.LayerNorm.bias\n",
      "name bert.encoder.layer.7.attention.self.query.weight\n",
      "name bert.encoder.layer.7.attention.self.query.bias\n",
      "name bert.encoder.layer.7.attention.self.key.weight\n",
      "name bert.encoder.layer.7.attention.self.key.bias\n",
      "name bert.encoder.layer.7.attention.self.value.weight\n",
      "name bert.encoder.layer.7.attention.self.value.bias\n",
      "name bert.encoder.layer.7.attention.output.dense.weight\n",
      "name bert.encoder.layer.7.attention.output.dense.bias\n",
      "name bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.7.intermediate.dense.weight\n",
      "name bert.encoder.layer.7.intermediate.dense.bias\n",
      "name bert.encoder.layer.7.output.dense.weight\n",
      "name bert.encoder.layer.7.output.dense.bias\n",
      "name bert.encoder.layer.7.output.LayerNorm.weight\n",
      "name bert.encoder.layer.7.output.LayerNorm.bias\n",
      "name bert.encoder.layer.8.attention.self.query.weight\n",
      "name bert.encoder.layer.8.attention.self.query.bias\n",
      "name bert.encoder.layer.8.attention.self.key.weight\n",
      "name bert.encoder.layer.8.attention.self.key.bias\n",
      "name bert.encoder.layer.8.attention.self.value.weight\n",
      "name bert.encoder.layer.8.attention.self.value.bias\n",
      "name bert.encoder.layer.8.attention.output.dense.weight\n",
      "name bert.encoder.layer.8.attention.output.dense.bias\n",
      "name bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.8.intermediate.dense.weight\n",
      "name bert.encoder.layer.8.intermediate.dense.bias\n",
      "name bert.encoder.layer.8.output.dense.weight\n",
      "name bert.encoder.layer.8.output.dense.bias\n",
      "name bert.encoder.layer.8.output.LayerNorm.weight\n",
      "name bert.encoder.layer.8.output.LayerNorm.bias\n",
      "name bert.encoder.layer.9.attention.self.query.weight\n",
      "name bert.encoder.layer.9.attention.self.query.bias\n",
      "name bert.encoder.layer.9.attention.self.key.weight\n",
      "name bert.encoder.layer.9.attention.self.key.bias\n",
      "name bert.encoder.layer.9.attention.self.value.weight\n",
      "name bert.encoder.layer.9.attention.self.value.bias\n",
      "name bert.encoder.layer.9.attention.output.dense.weight\n",
      "name bert.encoder.layer.9.attention.output.dense.bias\n",
      "name bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.9.intermediate.dense.weight\n",
      "name bert.encoder.layer.9.intermediate.dense.bias\n",
      "name bert.encoder.layer.9.output.dense.weight\n",
      "name bert.encoder.layer.9.output.dense.bias\n",
      "name bert.encoder.layer.9.output.LayerNorm.weight\n",
      "name bert.encoder.layer.9.output.LayerNorm.bias\n",
      "name bert.encoder.layer.10.attention.self.query.weight\n",
      "name bert.encoder.layer.10.attention.self.query.bias\n",
      "name bert.encoder.layer.10.attention.self.key.weight\n",
      "name bert.encoder.layer.10.attention.self.key.bias\n",
      "name bert.encoder.layer.10.attention.self.value.weight\n",
      "name bert.encoder.layer.10.attention.self.value.bias\n",
      "name bert.encoder.layer.10.attention.output.dense.weight\n",
      "name bert.encoder.layer.10.attention.output.dense.bias\n",
      "name bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.10.intermediate.dense.weight\n",
      "name bert.encoder.layer.10.intermediate.dense.bias\n",
      "name bert.encoder.layer.10.output.dense.weight\n",
      "name bert.encoder.layer.10.output.dense.bias\n",
      "name bert.encoder.layer.10.output.LayerNorm.weight\n",
      "name bert.encoder.layer.10.output.LayerNorm.bias\n",
      "name bert.encoder.layer.11.attention.self.query.weight\n",
      "name bert.encoder.layer.11.attention.self.query.bias\n",
      "name bert.encoder.layer.11.attention.self.key.weight\n",
      "name bert.encoder.layer.11.attention.self.key.bias\n",
      "name bert.encoder.layer.11.attention.self.value.weight\n",
      "name bert.encoder.layer.11.attention.self.value.bias\n",
      "name bert.encoder.layer.11.attention.output.dense.weight\n",
      "name bert.encoder.layer.11.attention.output.dense.bias\n",
      "name bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.11.intermediate.dense.weight\n",
      "name bert.encoder.layer.11.intermediate.dense.bias\n",
      "name bert.encoder.layer.11.output.dense.weight\n",
      "name bert.encoder.layer.11.output.dense.bias\n",
      "name bert.encoder.layer.11.output.LayerNorm.weight\n",
      "name bert.encoder.layer.11.output.LayerNorm.bias\n",
      "name bert.pooler.dense.weight\n",
      "name bert.pooler.dense.bias\n",
      "name cls.predictions.bias\n",
      "name cls.predictions.transform.dense.weight\n",
      "name cls.predictions.transform.dense.bias\n",
      "name cls.predictions.transform.LayerNorm.weight\n",
      "name cls.predictions.transform.LayerNorm.bias\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "print(model)\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "if torch.cuda.is_available() == True:\n",
    "    tokens_tensor = tokens_tensor.to('cuda')\n",
    "    segments_tensors = segments_tensors.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "    print(predictions.size())\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'henson'\n",
    "print(predicted_index)\n",
    "print(predicted_token)\n",
    "\n",
    "for (name, param) in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print('name', name#, 'weigth data', param.data\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "name fc.weight weigth data tensor([[ 0.0395,  0.0001, -0.0398,  ..., -0.0137,  0.0151, -0.0033],\n",
      "        [ 0.0112, -0.0344,  0.0121,  ...,  0.0254, -0.0233,  0.0096],\n",
      "        [-0.0254,  0.0164, -0.0430,  ...,  0.0147, -0.0373,  0.0021],\n",
      "        ...,\n",
      "        [ 0.0369,  0.0255,  0.0124,  ..., -0.0060,  0.0034,  0.0264],\n",
      "        [-0.0164,  0.0205,  0.0350,  ..., -0.0397,  0.0283, -0.0100],\n",
      "        [ 0.0056,  0.0002,  0.0368,  ..., -0.0351, -0.0242,  0.0194]])\n",
      "name fc.bias weigth data tensor([-0.0327,  0.0078,  0.0236,  0.0180, -0.0362, -0.0247,  0.0032, -0.0158])\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.vgg19(pretrained=True)\n",
    "print(model)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    # Replace the last fully-connected layer\n",
    "    # Parameters of newly constructed modules have requires_grad=True by default\n",
    "model.fc = nn.Linear(512, 8) # assuming that the fc7 layer has 512 neurons, otherwise change it \n",
    "# model.fc\n",
    "# model.cuda()\n",
    "# model.classifier[6] = nn.Linear(512, 8)\n",
    "# model.fc\n",
    "for (name, param) in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print('name', name, 'weigth data', param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
