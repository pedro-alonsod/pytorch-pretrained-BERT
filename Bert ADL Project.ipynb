{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.circleci', '.git', '.github', '.gitignore', '.ipynb_checkpoints', 'askreddit_data.json', 'Bert ADL Project.ipynb', 'CONTRIBUTING.md', 'convert_tf_checkpoint_to_pytorch.py', 'data', 'dataset.py', 'detokenization.py', 'docker', 'docs', 'examples', 'file_0.csv', 'hubconf.py', 'hubconfs', 'LICENSE', 'MANIFEST.in', 'masked_language_model.py', 'modeling.py', 'notebooks', 'optimization.py', 'outputs.html', 'pytorch_pretrained_bert', 'README.md', 'requirements.txt', 'samples', 'setup.py', 'tests', 'test_threads.txt', 'tokenization.py', 'train_threads.txt', 'uncased_askreddit_data.tsv', '__init__.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "# from torchsummary import summary\n",
    "import tokenization\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import logging\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import six\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchtext import data,datasets\n",
    "from torchtext.vocab import GloVe,FastText,CharNGram\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import sys\n",
    "import torchtext.data.dataset\n",
    "\n",
    "import tokenization\n",
    "from modeling import BertConfig, BertForMaskedLanguageModelling\n",
    "from optimization import BERTAdam\n",
    "from masked_language_model import notqdm, convert_tokens_to_features, LMProcessor, predict_masked_words, predict_next_words, improve_words_recursive\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "import csv\n",
    "\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('./askreddit_data.json', 'r'))\n",
    "\n",
    "# ask_reddit_data =  pd.read_json('./askreddit_data.json')\n",
    "# ask_reddit_data = json_normalize(data)\n",
    "uncased_ask_reddit_data = pd.read_csv('./uncased_askreddit_data.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission score</th>\n",
       "      <th>comment score</th>\n",
       "      <th>submission id</th>\n",
       "      <th>comment id</th>\n",
       "      <th>title (string #1)</th>\n",
       "      <th>comment (string#2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80513</td>\n",
       "      <td>25495</td>\n",
       "      <td>bn3bpz</td>\n",
       "      <td>en27b8v</td>\n",
       "      <td>would you support a mandatory environmental se...</td>\n",
       "      <td>not mandatory, but there was a voluntary progr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80513</td>\n",
       "      <td>4686</td>\n",
       "      <td>bn3bpz</td>\n",
       "      <td>en2anug</td>\n",
       "      <td>would you support a mandatory environmental se...</td>\n",
       "      <td>i feel like instead of forcing people to do so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80513</td>\n",
       "      <td>2041</td>\n",
       "      <td>bn3bpz</td>\n",
       "      <td>en254lj</td>\n",
       "      <td>would you support a mandatory environmental se...</td>\n",
       "      <td>i wouldn't make it mandatory but maybe optiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80513</td>\n",
       "      <td>608</td>\n",
       "      <td>bn3bpz</td>\n",
       "      <td>en2gacy</td>\n",
       "      <td>would you support a mandatory environmental se...</td>\n",
       "      <td>we already have americorps, where you serve fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80513</td>\n",
       "      <td>310</td>\n",
       "      <td>bn3bpz</td>\n",
       "      <td>en2p5ro</td>\n",
       "      <td>would you support a mandatory environmental se...</td>\n",
       "      <td>i feel like forcing people to do it will just ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   submission score  comment score submission id comment id  \\\n",
       "0             80513          25495        bn3bpz    en27b8v   \n",
       "1             80513           4686        bn3bpz    en2anug   \n",
       "2             80513           2041        bn3bpz    en254lj   \n",
       "3             80513            608        bn3bpz    en2gacy   \n",
       "4             80513            310        bn3bpz    en2p5ro   \n",
       "\n",
       "                                   title (string #1)  \\\n",
       "0  would you support a mandatory environmental se...   \n",
       "1  would you support a mandatory environmental se...   \n",
       "2  would you support a mandatory environmental se...   \n",
       "3  would you support a mandatory environmental se...   \n",
       "4  would you support a mandatory environmental se...   \n",
       "\n",
       "                                  comment (string#2)  \n",
       "0  not mandatory, but there was a voluntary progr...  \n",
       "1  i feel like instead of forcing people to do so...  \n",
       "2  i wouldn't make it mandatory but maybe optiona...  \n",
       "3  we already have americorps, where you serve fo...  \n",
       "4  i feel like forcing people to do it will just ...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncased_ask_reddit_data.head()\n",
    "# uncased_ask_reddit_data['submission id'] == 'bn3bpz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bncqlb', 'bq7pbb', 'btexev', 'bls9ab', 'bpr8xn', 'bri3ja', 'bojhpr', 'bqltzq', 'blrjnj', 'bqhfq9', 'bskplh', 'bkxl3u', 'bmhq67', 'bty7a5', 'bkjqjj', 'bmt72c', 'bubm6g', 'bn3ab7', 'bvgvkh', 'bq2sd5', 'btfu8k', 'buz6lq', 'br87g0', 'bovsdb', 'bu1s5i', 'bs3th0', 'blna2q', 'brzuwa', 'bqx0lw', 'bvr285', 'bk8ypo', 'boxktx', 'br19bh', 'boi2wy', 'bogywr', 'bo3owh', 'blzj3m', 'bo6854', 'bmhwr8', 'bobtwf', 'bmjk7w', 'brlti4', 'buso57', 'bv51tj', 'bt3zc7', 'bsg50t', 'bstmp7', 'blf1l9', 'bvdaci', 'bmyv30', 'bm8dfx', 'bnlib2', 'bn3bpz', 'bp9xm7', 'bp182w', 'bkh4qb', 'bpw8r2', 'bniexw', 'bljq8j', 'bs6tqc', 'bro41c', 'bsahi8', 'btm0yy', 'bue7cv', 'bsomsy', 'bqc3ib', 'brayzq', 'bu766u', 'bkkoq2', 'bntob1', 'budsip', 'bqg2xi', 'bltmmq', 'buluna', 'bmac4g', 'bv6o78', 'btb5si', 'bndpre', 'buunu0', 'bkckiw', 'bkseqj', 'bugoyw', 'bl2scw', 'bmlzqs', 'bnqk9j', 'bkw3yd', 'bvkru1', 'blcbrl', 'bk6w6e', 'bquclv', 'bttd2p'} 91\n",
      "['bmjk7w', 'br87g0', 'bkh4qb', 'bk6w6e', 'bncqlb', 'blna2q', 'bobtwf', 'brayzq', 'bskplh', 'bt3zc7'] 10\n"
     ]
    }
   ],
   "source": [
    "array_id = uncased_ask_reddit_data['submission id']\n",
    "\n",
    "set_of_ids = set(array_id)\n",
    "\n",
    "print(set_of_ids, len(set_of_ids))\n",
    "\n",
    "file_train, file_test = train_test_split(list(set_of_ids), random_state=42, test_size=0.10)\n",
    "print(file_test, len(file_test))\n",
    "# print(file_train, len(file_train))\n",
    "# with open('train_threads.txt', 'a+') as file_train_ids:\n",
    "#     for elem in file_train:\n",
    "#         file_train_ids.write(elem + '\\n')\n",
    "\n",
    "# # with open('test_threads.txt', 'a+') as file_test_ids:\n",
    "#     for elem in file_test:\n",
    "#         file_test_ids.write(elem + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncased_ask_reddit_data['submission id'] == 'bn3bpz'\n",
    "# uncased_ask_reddit_data.loc[uncased_ask_reddit_data['submission id'] == 'bn3bpz']\n",
    "train_df = [] #pd.DataFrame(columns=['submission score\tcomment score', 'submission id', 'comment id', 'title (string #1)', 'comment (string#2)'])\n",
    "test_df = []\n",
    "\n",
    "with open('train_threads.txt', 'r') as trt:\n",
    "    for elem in trt.readlines():\n",
    "#         print(elem.strip())\n",
    "#         print(uncased_ask_reddit_data.loc[uncased_ask_reddit_data['submission id'] == elem.strip()])\n",
    "        train_df.append(uncased_ask_reddit_data.loc[uncased_ask_reddit_data['submission id'] == elem.strip()])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_threads.txt', 'r') as tst:\n",
    "    for elem in tst.readlines():\n",
    "#         print(elem.strip())\n",
    "#         print(uncased_ask_reddit_data.loc[uncased_ask_reddit_data['submission id'] == elem.strip()])\n",
    "        test_df.append(uncased_ask_reddit_data.loc[uncased_ask_reddit_data['submission id'] == elem.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_df[:3])\n",
    "# train_df_parsed = pd.DataFrame(train_df[0])\n",
    "# train_df_parsed.to_csv(path_or_buf='./file_0.csv', sep='\\t')\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    train_df_parsed = pd.DataFrame(train_df[i])\n",
    "    train_df_parsed.to_csv(path_or_buf='./data/train/file_'+str(i)+'.csv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_df)):\n",
    "    test_df_parsed = pd.DataFrame(train_df[i])\n",
    "    test_df_parsed.to_csv(path_or_buf='./data/test/file_'+str(i)+'.csv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     submission score comment score submission id comment id  \\\n",
      "0               29117          6431        bncqlb    en4pxcp   \n",
      "1               29117         24298        bncqlb    en4hp28   \n",
      "2               29117         15415        bncqlb    en4kqq9   \n",
      "3               29117         14389        bncqlb    en4f7vh   \n",
      "4               29117          5357        bncqlb    en4m0gm   \n",
      "5               29117         16081        bncqlb    en4hvq1   \n",
      "6               29117         25806        bncqlb    en4n6hx   \n",
      "7               29117          4892        bncqlb    en4izto   \n",
      "8               29117         12290        bncqlb    en4hijc   \n",
      "9               29117         17502        bncqlb    en4lisi   \n",
      "10              29117         19126        bncqlb    en4et42   \n",
      "11              29117         16508        bncqlb    en4je3s   \n",
      "12              29117          4241        bncqlb    en4fcs2   \n",
      "13              29117          6952        bncqlb    en4ip4o   \n",
      "14              29117          2167        bncqlb    en4kyct   \n",
      "15              29117          1294        bncqlb    en4kjtk   \n",
      "16              29117          2542        bncqlb    en4s2a4   \n",
      "17              29117         10935        bncqlb    en4kiqm   \n",
      "18              29117          1159        bncqlb    en4okfv   \n",
      "19              29117          2690        bncqlb    en4ivl5   \n",
      "20              29117          2151        bncqlb    en4f8k1   \n",
      "21              29117          2549        bncqlb    en4fces   \n",
      "22              29117          1292        bncqlb    en4gg76   \n",
      "23              29117          1731        bncqlb    en4jxag   \n",
      "24              29117          1792        bncqlb    en4f1lf   \n",
      "25              29117           583        bncqlb    en4izte   \n",
      "26              29117           716        bncqlb    en4fn6b   \n",
      "27              29117           933        bncqlb    en4kra7   \n",
      "28              29117           145        bncqlb    en4maeb   \n",
      "29              29117           287        bncqlb    en4so8x   \n",
      "...               ...           ...           ...        ...   \n",
      "7174            36634           348        bmac4g    emvbygr   \n",
      "7175            36634           280        bmac4g    emv55qy   \n",
      "7176            36634           739        bmac4g    emvadgs   \n",
      "7177            36634          2028        bmac4g    emv6o43   \n",
      "7178            36634           127        bmac4g    emvdonh   \n",
      "7179            36634           223        bmac4g    emvhewk   \n",
      "7180            36634           210        bmac4g    emvbldp   \n",
      "7181            36634           503        bmac4g    emvixj5   \n",
      "7182            36634           287        bmac4g    emvnobt   \n",
      "7183            36634          1483        bmac4g    emvb4m7   \n",
      "7184            36634           103        bmac4g    emvkedl   \n",
      "7185            36634            48        bmac4g    emvvi1l   \n",
      "7186            36634           186        bmac4g    emvqqr5   \n",
      "7187            36634           120        bmac4g    emvazr6   \n",
      "7188            36634           435        bmac4g    emvk2xr   \n",
      "7189            36634            84        bmac4g    emvcfxv   \n",
      "7190            36634            79        bmac4g    emw18zn   \n",
      "7191            36634           165        bmac4g    emvdrot   \n",
      "7192            36634           592        bmac4g    emv9vyv   \n",
      "7193            36634            35        bmac4g    emvz0z4   \n",
      "7194            36634           561        bmac4g    emv1zia   \n",
      "7195            36634           183        bmac4g    emv97oh   \n",
      "7196            36634           263        bmac4g    emvjv8h   \n",
      "7197            36634           166        bmac4g    emv8jnq   \n",
      "7198            36634            31        bmac4g    emw12su   \n",
      "7199            36634           135        bmac4g    emv5y78   \n",
      "7200            36634           111        bmac4g    emv5kcm   \n",
      "7201            36634           121        bmac4g    emvg938   \n",
      "7202            36634            29        bmac4g    emw7f18   \n",
      "7203            36634            28        bmac4g    emw8air   \n",
      "\n",
      "                                      title (string #1)  \\\n",
      "0     if movie titles were taken literally, what blo...   \n",
      "1     if movie titles were taken literally, what blo...   \n",
      "2     if movie titles were taken literally, what blo...   \n",
      "3     if movie titles were taken literally, what blo...   \n",
      "4     if movie titles were taken literally, what blo...   \n",
      "5     if movie titles were taken literally, what blo...   \n",
      "6     if movie titles were taken literally, what blo...   \n",
      "7     if movie titles were taken literally, what blo...   \n",
      "8     if movie titles were taken literally, what blo...   \n",
      "9     if movie titles were taken literally, what blo...   \n",
      "10    if movie titles were taken literally, what blo...   \n",
      "11    if movie titles were taken literally, what blo...   \n",
      "12    if movie titles were taken literally, what blo...   \n",
      "13    if movie titles were taken literally, what blo...   \n",
      "14    if movie titles were taken literally, what blo...   \n",
      "15    if movie titles were taken literally, what blo...   \n",
      "16    if movie titles were taken literally, what blo...   \n",
      "17    if movie titles were taken literally, what blo...   \n",
      "18    if movie titles were taken literally, what blo...   \n",
      "19    if movie titles were taken literally, what blo...   \n",
      "20    if movie titles were taken literally, what blo...   \n",
      "21    if movie titles were taken literally, what blo...   \n",
      "22    if movie titles were taken literally, what blo...   \n",
      "23    if movie titles were taken literally, what blo...   \n",
      "24    if movie titles were taken literally, what blo...   \n",
      "25    if movie titles were taken literally, what blo...   \n",
      "26    if movie titles were taken literally, what blo...   \n",
      "27    if movie titles were taken literally, what blo...   \n",
      "28    if movie titles were taken literally, what blo...   \n",
      "29    if movie titles were taken literally, what blo...   \n",
      "...                                                 ...   \n",
      "7174  whats something that cant be explained, it mus...   \n",
      "7175  whats something that cant be explained, it mus...   \n",
      "7176  whats something that cant be explained, it mus...   \n",
      "7177  whats something that cant be explained, it mus...   \n",
      "7178  whats something that cant be explained, it mus...   \n",
      "7179  whats something that cant be explained, it mus...   \n",
      "7180  whats something that cant be explained, it mus...   \n",
      "7181  whats something that cant be explained, it mus...   \n",
      "7182  whats something that cant be explained, it mus...   \n",
      "7183  whats something that cant be explained, it mus...   \n",
      "7184  whats something that cant be explained, it mus...   \n",
      "7185  whats something that cant be explained, it mus...   \n",
      "7186  whats something that cant be explained, it mus...   \n",
      "7187  whats something that cant be explained, it mus...   \n",
      "7188  whats something that cant be explained, it mus...   \n",
      "7189  whats something that cant be explained, it mus...   \n",
      "7190  whats something that cant be explained, it mus...   \n",
      "7191  whats something that cant be explained, it mus...   \n",
      "7192  whats something that cant be explained, it mus...   \n",
      "7193  whats something that cant be explained, it mus...   \n",
      "7194  whats something that cant be explained, it mus...   \n",
      "7195  whats something that cant be explained, it mus...   \n",
      "7196  whats something that cant be explained, it mus...   \n",
      "7197  whats something that cant be explained, it mus...   \n",
      "7198  whats something that cant be explained, it mus...   \n",
      "7199  whats something that cant be explained, it mus...   \n",
      "7200  whats something that cant be explained, it mus...   \n",
      "7201  whats something that cant be explained, it mus...   \n",
      "7202  whats something that cant be explained, it mus...   \n",
      "7203  whats something that cant be explained, it mus...   \n",
      "\n",
      "                                     comment (string#2)  \n",
      "0     mission impossible. \"sir, this mission is impo...  \n",
      "1                              the silence of the lambs  \n",
      "2                                              the rock  \n",
      "3                                     50 shades of grey  \n",
      "4                              meet joe black hi. (fin)  \n",
      "5                                           silent hill  \n",
      "6     the fifth element - it would be a two hours lo...  \n",
      "7                                             rush hour  \n",
      "8                                men who stare at goats  \n",
      "9     \"gone in 60 seconds\" it's just the movie \"gone...  \n",
      "10    law abiding citizen. \"local man pays his taxes...  \n",
      "11                                       the green mile  \n",
      "12                                            127 hours  \n",
      "13                                        a quiet place  \n",
      "14                                      ordinary people  \n",
      "15                                              waiting  \n",
      "16                                  night at the museum  \n",
      "17    home alone: 90 minutes of a guy on a couch wat...  \n",
      "18    milk. just an hour-long still shot of a cold g...  \n",
      "19    the shining. watch out. you'll need a pair of ...  \n",
      "20    transformers- would be about a guy who goes ar...  \n",
      "21                              much ado about nothing.  \n",
      "22                                      friday the 13th  \n",
      "23    mib- it's just an hour of a half of men being ...  \n",
      "24    i'll say jaws. how many mandibles would i be f...  \n",
      "25    independence day. went to see the folks, ate s...  \n",
      "26          dead snow. just a movie about snow melting.  \n",
      "27                                                   it  \n",
      "28            top gun. (places pistol on the top shelf)  \n",
      "29                                    neverending story  \n",
      "...                                                 ...  \n",
      "7174                                     lucid dreaming  \n",
      "7175                                              mania  \n",
      "7176  finally feeling like you are an adult. the bes...  \n",
      "7177            reddit to people that don't use reddit.  \n",
      "7178  the feeling of being shot. it seems to vary so...  \n",
      "7179  having a dad thats there for you for the first...  \n",
      "7180  breaking up with a partner you still love. tyi...  \n",
      "7181  being in an abusive relationship and not reali...  \n",
      "7182  adhd. you can tell people how you feel and how...  \n",
      "7183  people look at me funny when i say this, but c...  \n",
      "7184  seeing mountains for the first time. pictures ...  \n",
      "7185  migraines. \"imagine a really bad headache on o...  \n",
      "7186  having milk come out of your boobs. there is n...  \n",
      "7187  skydiving. feeling the stomach churning drop a...  \n",
      "7188  being a parent. you've spent your whole life a...  \n",
      "7189  bee stings . been stung 6 times but can never ...  \n",
      "7190     period cramps, its such a unique kind of pain.  \n",
      "7191  hugging your mom after a particularly long, ha...  \n",
      "7192                              why people like math.  \n",
      "7193  what its truly like to grieve for your departe...  \n",
      "7194  going to sleep after a physically exhausting d...  \n",
      "7195  marilyn monroe in movies. in pictures some peo...  \n",
      "7196  okay, disclaimer, dont experience this please....  \n",
      "7197  prostate orgasms for a guy. don't judge before...  \n",
      "7198  living alone for an *extremely* long time. i'v...  \n",
      "7199                                                dmt  \n",
      "7200    being out on a pitch in front of 50,000 people.  \n",
      "7201                                            shwarma  \n",
      "7202  before i became addicted to heroin i thought a...  \n",
      "7203  broccoli, more specifically after it has been ...  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7204 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "data_dict = dict()\n",
    "data_dict['submission score'] = []\n",
    "data_dict['comment score'] = []\n",
    "data_dict['submission id'] = []\n",
    "data_dict['comment id'] = []\n",
    "data_dict['title (string #1)'] = []\n",
    "data_dict['comment (string#2)'] = []\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "#     print(i, './data/train/file_'+str(i)+'.csv')\n",
    "    with open('./data/train/file_'+str(i)+'.csv') as tsvfile:\n",
    "        reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
    "    #     next(reader)\n",
    "    #     df = pd.DataFrame(reader.data, columns=readr.columns)\n",
    "    #     print(df.head())\n",
    "        for row in reader:\n",
    "    #         print(row)\n",
    "    #         data_dict += row\n",
    "            data_dict['submission score'].append(row['submission score'])\n",
    "            data_dict['comment score'].append(row['comment score'])\n",
    "            data_dict['submission id'].append(row['submission id'])\n",
    "            data_dict['comment id'].append(row['comment id'])\n",
    "            data_dict['title (string #1)'].append(row['title (string #1)'])\n",
    "            data_dict['comment (string#2)'].append(row['comment (string#2)'])\n",
    "        \n",
    "# print(data_dict.values())\n",
    "\n",
    "data_df_train = pd.DataFrame(data=data_dict)\n",
    "\n",
    "print(data_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df_train.to_csv('./data/TrainFile.csv', index=False, sep='\\t')\n",
    "# data_df_train['comment score'].to_csv('./data/labels.csv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    submission score comment score submission id comment id  \\\n",
      "0              29117          6431        bncqlb    en4pxcp   \n",
      "1              29117         24298        bncqlb    en4hp28   \n",
      "2              29117         15415        bncqlb    en4kqq9   \n",
      "3              29117         14389        bncqlb    en4f7vh   \n",
      "4              29117          5357        bncqlb    en4m0gm   \n",
      "5              29117         16081        bncqlb    en4hvq1   \n",
      "6              29117         25806        bncqlb    en4n6hx   \n",
      "7              29117          4892        bncqlb    en4izto   \n",
      "8              29117         12290        bncqlb    en4hijc   \n",
      "9              29117         17502        bncqlb    en4lisi   \n",
      "10             29117         19126        bncqlb    en4et42   \n",
      "11             29117         16508        bncqlb    en4je3s   \n",
      "12             29117          4241        bncqlb    en4fcs2   \n",
      "13             29117          6952        bncqlb    en4ip4o   \n",
      "14             29117          2167        bncqlb    en4kyct   \n",
      "15             29117          1294        bncqlb    en4kjtk   \n",
      "16             29117          2542        bncqlb    en4s2a4   \n",
      "17             29117         10935        bncqlb    en4kiqm   \n",
      "18             29117          1159        bncqlb    en4okfv   \n",
      "19             29117          2690        bncqlb    en4ivl5   \n",
      "20             29117          2151        bncqlb    en4f8k1   \n",
      "21             29117          2549        bncqlb    en4fces   \n",
      "22             29117          1292        bncqlb    en4gg76   \n",
      "23             29117          1731        bncqlb    en4jxag   \n",
      "24             29117          1792        bncqlb    en4f1lf   \n",
      "25             29117           583        bncqlb    en4izte   \n",
      "26             29117           716        bncqlb    en4fn6b   \n",
      "27             29117           933        bncqlb    en4kra7   \n",
      "28             29117           145        bncqlb    en4maeb   \n",
      "29             29117           287        bncqlb    en4so8x   \n",
      "..               ...           ...           ...        ...   \n",
      "914            18697         15145        btm0yy    eozqw7r   \n",
      "915            18697           414        btm0yy    eozvgnw   \n",
      "916            18697            97        btm0yy    ep07asi   \n",
      "917            18697            48        btm0yy    ep19ilb   \n",
      "918            18697           693        btm0yy    ep009s1   \n",
      "919            18697           449        btm0yy    eozop11   \n",
      "920            18697           269        btm0yy    eozyfd9   \n",
      "921            18697            77        btm0yy    ep0h021   \n",
      "922            18697           162        btm0yy    eozs7iq   \n",
      "923            18697           187        btm0yy    ep0322a   \n",
      "924            18697            70        btm0yy    ep0pnve   \n",
      "925            18697           118        btm0yy    ep08acu   \n",
      "926            18697            35        btm0yy    ep0dp8m   \n",
      "927            18697            35        btm0yy    ep0g6v2   \n",
      "928            18697            29        btm0yy    ep0i9f6   \n",
      "929            18697           393        btm0yy    ep010wv   \n",
      "930            18697            93        btm0yy    ep0vohd   \n",
      "931            18697            57        btm0yy    ep08ib5   \n",
      "932            18697            28        btm0yy    ep0119q   \n",
      "933            18697            29        btm0yy    ep0m2vn   \n",
      "934            18697            72        btm0yy    ep1cmms   \n",
      "935            18697            52        btm0yy    ep0939p   \n",
      "936            18697            24        btm0yy    ep17028   \n",
      "937            18697            80        btm0yy    ep05wri   \n",
      "938            18697            41        btm0yy    ep0i9nk   \n",
      "939            18697            46        btm0yy    ep13nnn   \n",
      "940            18697            22        btm0yy    ep1kzkk   \n",
      "941            18697            18        btm0yy    ep1t9ou   \n",
      "942            18697            51        btm0yy    ep0a6pa   \n",
      "943            18697            17        btm0yy    ep0j8nn   \n",
      "\n",
      "                                     title (string #1)  \\\n",
      "0    if movie titles were taken literally, what blo...   \n",
      "1    if movie titles were taken literally, what blo...   \n",
      "2    if movie titles were taken literally, what blo...   \n",
      "3    if movie titles were taken literally, what blo...   \n",
      "4    if movie titles were taken literally, what blo...   \n",
      "5    if movie titles were taken literally, what blo...   \n",
      "6    if movie titles were taken literally, what blo...   \n",
      "7    if movie titles were taken literally, what blo...   \n",
      "8    if movie titles were taken literally, what blo...   \n",
      "9    if movie titles were taken literally, what blo...   \n",
      "10   if movie titles were taken literally, what blo...   \n",
      "11   if movie titles were taken literally, what blo...   \n",
      "12   if movie titles were taken literally, what blo...   \n",
      "13   if movie titles were taken literally, what blo...   \n",
      "14   if movie titles were taken literally, what blo...   \n",
      "15   if movie titles were taken literally, what blo...   \n",
      "16   if movie titles were taken literally, what blo...   \n",
      "17   if movie titles were taken literally, what blo...   \n",
      "18   if movie titles were taken literally, what blo...   \n",
      "19   if movie titles were taken literally, what blo...   \n",
      "20   if movie titles were taken literally, what blo...   \n",
      "21   if movie titles were taken literally, what blo...   \n",
      "22   if movie titles were taken literally, what blo...   \n",
      "23   if movie titles were taken literally, what blo...   \n",
      "24   if movie titles were taken literally, what blo...   \n",
      "25   if movie titles were taken literally, what blo...   \n",
      "26   if movie titles were taken literally, what blo...   \n",
      "27   if movie titles were taken literally, what blo...   \n",
      "28   if movie titles were taken literally, what blo...   \n",
      "29   if movie titles were taken literally, what blo...   \n",
      "..                                                 ...   \n",
      "914  what is one moment when you realized you just ...   \n",
      "915  what is one moment when you realized you just ...   \n",
      "916  what is one moment when you realized you just ...   \n",
      "917  what is one moment when you realized you just ...   \n",
      "918  what is one moment when you realized you just ...   \n",
      "919  what is one moment when you realized you just ...   \n",
      "920  what is one moment when you realized you just ...   \n",
      "921  what is one moment when you realized you just ...   \n",
      "922  what is one moment when you realized you just ...   \n",
      "923  what is one moment when you realized you just ...   \n",
      "924  what is one moment when you realized you just ...   \n",
      "925  what is one moment when you realized you just ...   \n",
      "926  what is one moment when you realized you just ...   \n",
      "927  what is one moment when you realized you just ...   \n",
      "928  what is one moment when you realized you just ...   \n",
      "929  what is one moment when you realized you just ...   \n",
      "930  what is one moment when you realized you just ...   \n",
      "931  what is one moment when you realized you just ...   \n",
      "932  what is one moment when you realized you just ...   \n",
      "933  what is one moment when you realized you just ...   \n",
      "934  what is one moment when you realized you just ...   \n",
      "935  what is one moment when you realized you just ...   \n",
      "936  what is one moment when you realized you just ...   \n",
      "937  what is one moment when you realized you just ...   \n",
      "938  what is one moment when you realized you just ...   \n",
      "939  what is one moment when you realized you just ...   \n",
      "940  what is one moment when you realized you just ...   \n",
      "941  what is one moment when you realized you just ...   \n",
      "942  what is one moment when you realized you just ...   \n",
      "943  what is one moment when you realized you just ...   \n",
      "\n",
      "                                    comment (string#2)  \n",
      "0    mission impossible. \"sir, this mission is impo...  \n",
      "1                             the silence of the lambs  \n",
      "2                                             the rock  \n",
      "3                                    50 shades of grey  \n",
      "4                             meet joe black hi. (fin)  \n",
      "5                                          silent hill  \n",
      "6    the fifth element - it would be a two hours lo...  \n",
      "7                                            rush hour  \n",
      "8                               men who stare at goats  \n",
      "9    \"gone in 60 seconds\" it's just the movie \"gone...  \n",
      "10   law abiding citizen. \"local man pays his taxes...  \n",
      "11                                      the green mile  \n",
      "12                                           127 hours  \n",
      "13                                       a quiet place  \n",
      "14                                     ordinary people  \n",
      "15                                             waiting  \n",
      "16                                 night at the museum  \n",
      "17   home alone: 90 minutes of a guy on a couch wat...  \n",
      "18   milk. just an hour-long still shot of a cold g...  \n",
      "19   the shining. watch out. you'll need a pair of ...  \n",
      "20   transformers- would be about a guy who goes ar...  \n",
      "21                             much ado about nothing.  \n",
      "22                                     friday the 13th  \n",
      "23   mib- it's just an hour of a half of men being ...  \n",
      "24   i'll say jaws. how many mandibles would i be f...  \n",
      "25   independence day. went to see the folks, ate s...  \n",
      "26         dead snow. just a movie about snow melting.  \n",
      "27                                                  it  \n",
      "28           top gun. (places pistol on the top shelf)  \n",
      "29                                   neverending story  \n",
      "..                                                 ...  \n",
      "914  at one point a few years ago, my girlfriend (a...  \n",
      "915                when you say \"yes\" to the threesome  \n",
      "916  was talking about a huge project for a cs clas...  \n",
      "917  i agreed to see my dad for the first time in 1...  \n",
      "918  woke up in a small ditch on the side of a road...  \n",
      "919  that moment when you realized youve been using...  \n",
      "920  told the wrong girl i loved her and the next m...  \n",
      "921  text my friend to bitch about someone. sent th...  \n",
      "922  when the alarm clock didn't go off and out the...  \n",
      "923  with my ex. by not acknowledging my feelings a...  \n",
      "924  we were taking our s.a.ts and you are not allo...  \n",
      "925  im wearing beige colored pants. i am in the ba...  \n",
      "926  when i was younger, i used to jump from one so...  \n",
      "927  when i forgot that my ex still had my snapchat...  \n",
      "928           in second grade i called my teacher mom.  \n",
      "929  i got called into a meeting with my line manag...  \n",
      "930  telling a girl i have been seeing that i have ...  \n",
      "931  me and brother went on holiday together in ten...  \n",
      "932  when i looked up the kh2 walkthrough and reali...  \n",
      "933  walking into an academic appeal hearing, recog...  \n",
      "934  worked at a dubbing studio. took a pic of an v...  \n",
      "935  reaching for the cutoff valve on part of the p...  \n",
      "936  when i realized i was missing one of my 50 cra...  \n",
      "937  i got my pants pulled down at like a concert o...  \n",
      "938  when i was 7 my parents left me and my sister ...  \n",
      "939  i clogged the toilet at my boyfriend's house. ...  \n",
      "940  when my dad (who lived states away and wasn't ...  \n",
      "941  when my wife used my dead mother against me in...  \n",
      "942  i made one too many the room jokes during a ti...  \n",
      "943  right now, hopped up on percocet, sitting in t...  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[944 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "data_dict = dict()\n",
    "data_dict['submission score'] = []\n",
    "data_dict['comment score'] = []\n",
    "data_dict['submission id'] = []\n",
    "data_dict['comment id'] = []\n",
    "data_dict['title (string #1)'] = []\n",
    "data_dict['comment (string#2)'] = []\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "#     print(i, './data/train/file_'+str(i)+'.csv')\n",
    "    with open('./data/test/file_'+str(i)+'.csv') as tsvfile:\n",
    "        reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
    "    #     next(reader)\n",
    "    #     df = pd.DataFrame(reader.data, columns=readr.columns)\n",
    "    #     print(df.head())\n",
    "        for row in reader:\n",
    "    #         print(row)\n",
    "    #         data_dict += row\n",
    "            data_dict['submission score'].append(row['submission score'])\n",
    "            data_dict['comment score'].append(row['comment score'])\n",
    "            data_dict['submission id'].append(row['submission id'])\n",
    "            data_dict['comment id'].append(row['comment id'])\n",
    "            data_dict['title (string #1)'].append(row['title (string #1)'])\n",
    "            data_dict['comment (string#2)'].append(row['comment (string#2)'])\n",
    "        \n",
    "# print(data_dict.values())\n",
    "\n",
    "data_df_test = pd.DataFrame(data=data_dict)\n",
    "\n",
    "print(data_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df_test.to_csv('./data/TestFile.csv', index=False, sep='\\t')\n",
    "# data_df_test['comment score'].to_csv('./data/labelsTest.csv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\pedalo\\.cache\\torch\\pytorch_pretrained_bert\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', 'henson', 'was', 'a', 'puppet', '##eer', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenized input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2040,  2001,  3958, 27227,  1029,   102,  3958,   103,  2001,\n",
      "          1037, 13997, 11510,   102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "print(tokens_tensor)\n",
    "print(segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at C:\\Users\\pedalo\\.cache\\torch\\pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file C:\\Users\\pedalo\\.cache\\torch\\pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir C:\\Users\\pedalo\\AppData\\Local\\Temp\\tmpdz3jazde\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "if torch.cuda.is_available() == True:\n",
    "    tokens_tensor = tokens_tensor.to('cuda')\n",
    "    segments_tensors = segments_tensors.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "# We have a hidden states for each of the 12 layers in model bert-base-uncased\n",
    "assert len(encoded_layers) == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at C:\\Users\\pedalo\\.cache\\torch\\pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file C:\\Users\\pedalo\\.cache\\torch\\pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir C:\\Users\\pedalo\\AppData\\Local\\Temp\\tmp0pphl80j\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): BertLayerNorm()\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
      "    )\n",
      "  )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n",
      "torch.Size([1, 14, 30522])\n",
      "27227\n",
      "henson\n",
      "name bert.embeddings.word_embeddings.weight\n",
      "name bert.embeddings.position_embeddings.weight\n",
      "name bert.embeddings.token_type_embeddings.weight\n",
      "name bert.embeddings.LayerNorm.weight\n",
      "name bert.embeddings.LayerNorm.bias\n",
      "name bert.encoder.layer.0.attention.self.query.weight\n",
      "name bert.encoder.layer.0.attention.self.query.bias\n",
      "name bert.encoder.layer.0.attention.self.key.weight\n",
      "name bert.encoder.layer.0.attention.self.key.bias\n",
      "name bert.encoder.layer.0.attention.self.value.weight\n",
      "name bert.encoder.layer.0.attention.self.value.bias\n",
      "name bert.encoder.layer.0.attention.output.dense.weight\n",
      "name bert.encoder.layer.0.attention.output.dense.bias\n",
      "name bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.0.intermediate.dense.weight\n",
      "name bert.encoder.layer.0.intermediate.dense.bias\n",
      "name bert.encoder.layer.0.output.dense.weight\n",
      "name bert.encoder.layer.0.output.dense.bias\n",
      "name bert.encoder.layer.0.output.LayerNorm.weight\n",
      "name bert.encoder.layer.0.output.LayerNorm.bias\n",
      "name bert.encoder.layer.1.attention.self.query.weight\n",
      "name bert.encoder.layer.1.attention.self.query.bias\n",
      "name bert.encoder.layer.1.attention.self.key.weight\n",
      "name bert.encoder.layer.1.attention.self.key.bias\n",
      "name bert.encoder.layer.1.attention.self.value.weight\n",
      "name bert.encoder.layer.1.attention.self.value.bias\n",
      "name bert.encoder.layer.1.attention.output.dense.weight\n",
      "name bert.encoder.layer.1.attention.output.dense.bias\n",
      "name bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.1.intermediate.dense.weight\n",
      "name bert.encoder.layer.1.intermediate.dense.bias\n",
      "name bert.encoder.layer.1.output.dense.weight\n",
      "name bert.encoder.layer.1.output.dense.bias\n",
      "name bert.encoder.layer.1.output.LayerNorm.weight\n",
      "name bert.encoder.layer.1.output.LayerNorm.bias\n",
      "name bert.encoder.layer.2.attention.self.query.weight\n",
      "name bert.encoder.layer.2.attention.self.query.bias\n",
      "name bert.encoder.layer.2.attention.self.key.weight\n",
      "name bert.encoder.layer.2.attention.self.key.bias\n",
      "name bert.encoder.layer.2.attention.self.value.weight\n",
      "name bert.encoder.layer.2.attention.self.value.bias\n",
      "name bert.encoder.layer.2.attention.output.dense.weight\n",
      "name bert.encoder.layer.2.attention.output.dense.bias\n",
      "name bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.2.intermediate.dense.weight\n",
      "name bert.encoder.layer.2.intermediate.dense.bias\n",
      "name bert.encoder.layer.2.output.dense.weight\n",
      "name bert.encoder.layer.2.output.dense.bias\n",
      "name bert.encoder.layer.2.output.LayerNorm.weight\n",
      "name bert.encoder.layer.2.output.LayerNorm.bias\n",
      "name bert.encoder.layer.3.attention.self.query.weight\n",
      "name bert.encoder.layer.3.attention.self.query.bias\n",
      "name bert.encoder.layer.3.attention.self.key.weight\n",
      "name bert.encoder.layer.3.attention.self.key.bias\n",
      "name bert.encoder.layer.3.attention.self.value.weight\n",
      "name bert.encoder.layer.3.attention.self.value.bias\n",
      "name bert.encoder.layer.3.attention.output.dense.weight\n",
      "name bert.encoder.layer.3.attention.output.dense.bias\n",
      "name bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.3.intermediate.dense.weight\n",
      "name bert.encoder.layer.3.intermediate.dense.bias\n",
      "name bert.encoder.layer.3.output.dense.weight\n",
      "name bert.encoder.layer.3.output.dense.bias\n",
      "name bert.encoder.layer.3.output.LayerNorm.weight\n",
      "name bert.encoder.layer.3.output.LayerNorm.bias\n",
      "name bert.encoder.layer.4.attention.self.query.weight\n",
      "name bert.encoder.layer.4.attention.self.query.bias\n",
      "name bert.encoder.layer.4.attention.self.key.weight\n",
      "name bert.encoder.layer.4.attention.self.key.bias\n",
      "name bert.encoder.layer.4.attention.self.value.weight\n",
      "name bert.encoder.layer.4.attention.self.value.bias\n",
      "name bert.encoder.layer.4.attention.output.dense.weight\n",
      "name bert.encoder.layer.4.attention.output.dense.bias\n",
      "name bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.4.intermediate.dense.weight\n",
      "name bert.encoder.layer.4.intermediate.dense.bias\n",
      "name bert.encoder.layer.4.output.dense.weight\n",
      "name bert.encoder.layer.4.output.dense.bias\n",
      "name bert.encoder.layer.4.output.LayerNorm.weight\n",
      "name bert.encoder.layer.4.output.LayerNorm.bias\n",
      "name bert.encoder.layer.5.attention.self.query.weight\n",
      "name bert.encoder.layer.5.attention.self.query.bias\n",
      "name bert.encoder.layer.5.attention.self.key.weight\n",
      "name bert.encoder.layer.5.attention.self.key.bias\n",
      "name bert.encoder.layer.5.attention.self.value.weight\n",
      "name bert.encoder.layer.5.attention.self.value.bias\n",
      "name bert.encoder.layer.5.attention.output.dense.weight\n",
      "name bert.encoder.layer.5.attention.output.dense.bias\n",
      "name bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.5.intermediate.dense.weight\n",
      "name bert.encoder.layer.5.intermediate.dense.bias\n",
      "name bert.encoder.layer.5.output.dense.weight\n",
      "name bert.encoder.layer.5.output.dense.bias\n",
      "name bert.encoder.layer.5.output.LayerNorm.weight\n",
      "name bert.encoder.layer.5.output.LayerNorm.bias\n",
      "name bert.encoder.layer.6.attention.self.query.weight\n",
      "name bert.encoder.layer.6.attention.self.query.bias\n",
      "name bert.encoder.layer.6.attention.self.key.weight\n",
      "name bert.encoder.layer.6.attention.self.key.bias\n",
      "name bert.encoder.layer.6.attention.self.value.weight\n",
      "name bert.encoder.layer.6.attention.self.value.bias\n",
      "name bert.encoder.layer.6.attention.output.dense.weight\n",
      "name bert.encoder.layer.6.attention.output.dense.bias\n",
      "name bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.6.intermediate.dense.weight\n",
      "name bert.encoder.layer.6.intermediate.dense.bias\n",
      "name bert.encoder.layer.6.output.dense.weight\n",
      "name bert.encoder.layer.6.output.dense.bias\n",
      "name bert.encoder.layer.6.output.LayerNorm.weight\n",
      "name bert.encoder.layer.6.output.LayerNorm.bias\n",
      "name bert.encoder.layer.7.attention.self.query.weight\n",
      "name bert.encoder.layer.7.attention.self.query.bias\n",
      "name bert.encoder.layer.7.attention.self.key.weight\n",
      "name bert.encoder.layer.7.attention.self.key.bias\n",
      "name bert.encoder.layer.7.attention.self.value.weight\n",
      "name bert.encoder.layer.7.attention.self.value.bias\n",
      "name bert.encoder.layer.7.attention.output.dense.weight\n",
      "name bert.encoder.layer.7.attention.output.dense.bias\n",
      "name bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.7.intermediate.dense.weight\n",
      "name bert.encoder.layer.7.intermediate.dense.bias\n",
      "name bert.encoder.layer.7.output.dense.weight\n",
      "name bert.encoder.layer.7.output.dense.bias\n",
      "name bert.encoder.layer.7.output.LayerNorm.weight\n",
      "name bert.encoder.layer.7.output.LayerNorm.bias\n",
      "name bert.encoder.layer.8.attention.self.query.weight\n",
      "name bert.encoder.layer.8.attention.self.query.bias\n",
      "name bert.encoder.layer.8.attention.self.key.weight\n",
      "name bert.encoder.layer.8.attention.self.key.bias\n",
      "name bert.encoder.layer.8.attention.self.value.weight\n",
      "name bert.encoder.layer.8.attention.self.value.bias\n",
      "name bert.encoder.layer.8.attention.output.dense.weight\n",
      "name bert.encoder.layer.8.attention.output.dense.bias\n",
      "name bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.8.intermediate.dense.weight\n",
      "name bert.encoder.layer.8.intermediate.dense.bias\n",
      "name bert.encoder.layer.8.output.dense.weight\n",
      "name bert.encoder.layer.8.output.dense.bias\n",
      "name bert.encoder.layer.8.output.LayerNorm.weight\n",
      "name bert.encoder.layer.8.output.LayerNorm.bias\n",
      "name bert.encoder.layer.9.attention.self.query.weight\n",
      "name bert.encoder.layer.9.attention.self.query.bias\n",
      "name bert.encoder.layer.9.attention.self.key.weight\n",
      "name bert.encoder.layer.9.attention.self.key.bias\n",
      "name bert.encoder.layer.9.attention.self.value.weight\n",
      "name bert.encoder.layer.9.attention.self.value.bias\n",
      "name bert.encoder.layer.9.attention.output.dense.weight\n",
      "name bert.encoder.layer.9.attention.output.dense.bias\n",
      "name bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.9.intermediate.dense.weight\n",
      "name bert.encoder.layer.9.intermediate.dense.bias\n",
      "name bert.encoder.layer.9.output.dense.weight\n",
      "name bert.encoder.layer.9.output.dense.bias\n",
      "name bert.encoder.layer.9.output.LayerNorm.weight\n",
      "name bert.encoder.layer.9.output.LayerNorm.bias\n",
      "name bert.encoder.layer.10.attention.self.query.weight\n",
      "name bert.encoder.layer.10.attention.self.query.bias\n",
      "name bert.encoder.layer.10.attention.self.key.weight\n",
      "name bert.encoder.layer.10.attention.self.key.bias\n",
      "name bert.encoder.layer.10.attention.self.value.weight\n",
      "name bert.encoder.layer.10.attention.self.value.bias\n",
      "name bert.encoder.layer.10.attention.output.dense.weight\n",
      "name bert.encoder.layer.10.attention.output.dense.bias\n",
      "name bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.10.intermediate.dense.weight\n",
      "name bert.encoder.layer.10.intermediate.dense.bias\n",
      "name bert.encoder.layer.10.output.dense.weight\n",
      "name bert.encoder.layer.10.output.dense.bias\n",
      "name bert.encoder.layer.10.output.LayerNorm.weight\n",
      "name bert.encoder.layer.10.output.LayerNorm.bias\n",
      "name bert.encoder.layer.11.attention.self.query.weight\n",
      "name bert.encoder.layer.11.attention.self.query.bias\n",
      "name bert.encoder.layer.11.attention.self.key.weight\n",
      "name bert.encoder.layer.11.attention.self.key.bias\n",
      "name bert.encoder.layer.11.attention.self.value.weight\n",
      "name bert.encoder.layer.11.attention.self.value.bias\n",
      "name bert.encoder.layer.11.attention.output.dense.weight\n",
      "name bert.encoder.layer.11.attention.output.dense.bias\n",
      "name bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "name bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "name bert.encoder.layer.11.intermediate.dense.weight\n",
      "name bert.encoder.layer.11.intermediate.dense.bias\n",
      "name bert.encoder.layer.11.output.dense.weight\n",
      "name bert.encoder.layer.11.output.dense.bias\n",
      "name bert.encoder.layer.11.output.LayerNorm.weight\n",
      "name bert.encoder.layer.11.output.LayerNorm.bias\n",
      "name bert.pooler.dense.weight\n",
      "name bert.pooler.dense.bias\n",
      "name cls.predictions.bias\n",
      "name cls.predictions.transform.dense.weight\n",
      "name cls.predictions.transform.dense.bias\n",
      "name cls.predictions.transform.LayerNorm.weight\n",
      "name cls.predictions.transform.LayerNorm.bias\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "print(model)\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "if torch.cuda.is_available() == True:\n",
    "    tokens_tensor = tokens_tensor.to('cuda')\n",
    "    segments_tensors = segments_tensors.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "    print(predictions.size())\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'henson'\n",
    "print(predicted_index)\n",
    "print(predicted_token)\n",
    "\n",
    "for (name, param) in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print('name', name#, 'weigth data', param.data\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "name fc.weight weigth data tensor([[ 0.0395,  0.0001, -0.0398,  ..., -0.0137,  0.0151, -0.0033],\n",
      "        [ 0.0112, -0.0344,  0.0121,  ...,  0.0254, -0.0233,  0.0096],\n",
      "        [-0.0254,  0.0164, -0.0430,  ...,  0.0147, -0.0373,  0.0021],\n",
      "        ...,\n",
      "        [ 0.0369,  0.0255,  0.0124,  ..., -0.0060,  0.0034,  0.0264],\n",
      "        [-0.0164,  0.0205,  0.0350,  ..., -0.0397,  0.0283, -0.0100],\n",
      "        [ 0.0056,  0.0002,  0.0368,  ..., -0.0351, -0.0242,  0.0194]])\n",
      "name fc.bias weigth data tensor([-0.0327,  0.0078,  0.0236,  0.0180, -0.0362, -0.0247,  0.0032, -0.0158])\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.vgg19(pretrained=True)\n",
    "print(model)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    # Replace the last fully-connected layer\n",
    "    # Parameters of newly constructed modules have requires_grad=True by default\n",
    "model.fc = nn.Linear(512, 8) # assuming that the fc7 layer has 512 neurons, otherwise change it \n",
    "# model.fc\n",
    "# model.cuda()\n",
    "# model.classifier[6] = nn.Linear(512, 8)\n",
    "# model.fc\n",
    "for (name, param) in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print('name', name, 'weigth data', param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "564px",
    "left": "1158px",
    "right": "20px",
    "top": "106px",
    "width": "354px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
